{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "*Question*: how does generalization error (error on validation set) change as we reduce the amount of unique training examples for both the subspace-constrained and non-subspace-constrained methods?\n",
    "*Hypothesis*: Subspace constrained method will have less generalization error than non subspace constrained method as the number of unique training examples decreases\n",
    "*Todo*: use subspace dimension from experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.cnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "from copy import deepcopy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def rel_error(x, y):\n",
    "  # returns relative error\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "# Load the preprocessed CIFAR10 data.\n",
    "alldata = get_CIFAR10_data()\n",
    "for k, v in alldata.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- trial 0: 100 percent of data (49000 unique examples, 0 duplicate examples) -----\n",
      "=== TRAINING STANDARD MODEL FOR TRIAL 0 ===\n",
      "(Iteration 1 / 1960) loss: 2.304995\n",
      "(Epoch 0 / 2) train acc: 0.162000; val_acc: 0.175000\n",
      "(Iteration 21 / 1960) loss: 2.157250\n",
      "(Iteration 41 / 1960) loss: 1.967369\n",
      "(Iteration 61 / 1960) loss: 1.870945\n",
      "(Iteration 81 / 1960) loss: 1.716645\n",
      "(Iteration 101 / 1960) loss: 1.815589\n",
      "(Iteration 121 / 1960) loss: 1.715465\n",
      "(Iteration 141 / 1960) loss: 1.500106\n",
      "(Iteration 161 / 1960) loss: 1.874528\n",
      "(Iteration 181 / 1960) loss: 1.899476\n",
      "(Iteration 201 / 1960) loss: 1.594301\n",
      "(Iteration 221 / 1960) loss: 1.683984\n",
      "(Iteration 241 / 1960) loss: 1.429353\n",
      "(Iteration 261 / 1960) loss: 1.581854\n",
      "(Iteration 281 / 1960) loss: 1.497817\n",
      "(Iteration 301 / 1960) loss: 1.701874\n",
      "(Iteration 321 / 1960) loss: 1.597978\n",
      "(Iteration 341 / 1960) loss: 1.287961\n",
      "(Iteration 361 / 1960) loss: 1.658388\n",
      "(Iteration 381 / 1960) loss: 1.421290\n",
      "(Iteration 401 / 1960) loss: 1.529719\n",
      "(Iteration 421 / 1960) loss: 1.373857\n",
      "(Iteration 441 / 1960) loss: 1.469273\n",
      "(Iteration 461 / 1960) loss: 1.527788\n",
      "(Iteration 481 / 1960) loss: 1.453465\n",
      "(Iteration 501 / 1960) loss: 1.328216\n",
      "(Iteration 521 / 1960) loss: 1.733220\n",
      "(Iteration 541 / 1960) loss: 1.408056\n",
      "(Iteration 561 / 1960) loss: 1.425127\n",
      "(Iteration 581 / 1960) loss: 1.472533\n",
      "(Iteration 601 / 1960) loss: 1.345400\n",
      "(Iteration 621 / 1960) loss: 1.382454\n",
      "(Iteration 641 / 1960) loss: 1.370331\n",
      "(Iteration 661 / 1960) loss: 1.398574\n",
      "(Iteration 681 / 1960) loss: 1.667687\n",
      "(Iteration 701 / 1960) loss: 1.492465\n",
      "(Iteration 721 / 1960) loss: 1.189837\n",
      "(Iteration 741 / 1960) loss: 1.578752\n",
      "(Iteration 761 / 1960) loss: 1.442947\n",
      "(Iteration 781 / 1960) loss: 1.328164\n",
      "(Iteration 801 / 1960) loss: 1.376959\n",
      "(Iteration 821 / 1960) loss: 1.209001\n",
      "(Iteration 841 / 1960) loss: 1.205084\n",
      "(Iteration 861 / 1960) loss: 1.411246\n",
      "(Iteration 881 / 1960) loss: 1.304529\n",
      "(Iteration 901 / 1960) loss: 1.105571\n",
      "(Iteration 921 / 1960) loss: 1.224308\n",
      "(Iteration 941 / 1960) loss: 1.273890\n",
      "(Iteration 961 / 1960) loss: 1.155699\n",
      "(Epoch 1 / 2) train acc: 0.569000; val_acc: 0.566000\n",
      "(Iteration 981 / 1960) loss: 1.293258\n",
      "(Iteration 1001 / 1960) loss: 1.120955\n",
      "(Iteration 1021 / 1960) loss: 1.298117\n",
      "(Iteration 1041 / 1960) loss: 1.011012\n",
      "(Iteration 1061 / 1960) loss: 1.079536\n",
      "(Iteration 1081 / 1960) loss: 1.087849\n",
      "(Iteration 1101 / 1960) loss: 1.091478\n",
      "(Iteration 1121 / 1960) loss: 1.144640\n",
      "(Iteration 1141 / 1960) loss: 1.001573\n",
      "(Iteration 1161 / 1960) loss: 1.276534\n",
      "(Iteration 1181 / 1960) loss: 1.200896\n",
      "(Iteration 1201 / 1960) loss: 1.422796\n",
      "(Iteration 1221 / 1960) loss: 0.980099\n",
      "(Iteration 1241 / 1960) loss: 1.298889\n",
      "(Iteration 1261 / 1960) loss: 1.123039\n",
      "(Iteration 1281 / 1960) loss: 1.139953\n",
      "(Iteration 1301 / 1960) loss: 1.447189\n",
      "(Iteration 1321 / 1960) loss: 1.173175\n",
      "(Iteration 1341 / 1960) loss: 1.576062\n",
      "(Iteration 1361 / 1960) loss: 1.005977\n",
      "(Iteration 1381 / 1960) loss: 1.373469\n",
      "(Iteration 1401 / 1960) loss: 1.285416\n",
      "(Iteration 1421 / 1960) loss: 1.402448\n",
      "(Iteration 1441 / 1960) loss: 1.149117\n",
      "(Iteration 1461 / 1960) loss: 1.457785\n",
      "(Iteration 1481 / 1960) loss: 1.125473\n",
      "(Iteration 1501 / 1960) loss: 1.042109\n",
      "(Iteration 1521 / 1960) loss: 1.125542\n",
      "(Iteration 1541 / 1960) loss: 1.120643\n",
      "(Iteration 1561 / 1960) loss: 1.141236\n",
      "(Iteration 1581 / 1960) loss: 0.973537\n",
      "(Iteration 1601 / 1960) loss: 1.063208\n",
      "(Iteration 1621 / 1960) loss: 0.982119\n",
      "(Iteration 1641 / 1960) loss: 1.092574\n",
      "(Iteration 1661 / 1960) loss: 1.230010\n",
      "(Iteration 1681 / 1960) loss: 1.336258\n",
      "(Iteration 1701 / 1960) loss: 1.077776\n",
      "(Iteration 1721 / 1960) loss: 1.185332\n",
      "(Iteration 1741 / 1960) loss: 0.955485\n",
      "(Iteration 1761 / 1960) loss: 1.145032\n",
      "(Iteration 1781 / 1960) loss: 1.158497\n",
      "(Iteration 1801 / 1960) loss: 1.007584\n",
      "(Iteration 1821 / 1960) loss: 1.217505\n",
      "(Iteration 1841 / 1960) loss: 1.057309\n",
      "(Iteration 1861 / 1960) loss: 0.898611\n",
      "(Iteration 1881 / 1960) loss: 1.098192\n",
      "(Iteration 1901 / 1960) loss: 1.047646\n",
      "(Iteration 1921 / 1960) loss: 1.046244\n",
      "(Iteration 1941 / 1960) loss: 1.299715\n",
      "(Epoch 2 / 2) train acc: 0.632000; val_acc: 0.611000\n",
      "final accuracy for 100 percent of data, standard model: 0.6320 (train), 0.0000 (test)\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 0 ===\n",
      "(Iteration 1 / 1960) loss: 2.304869\n",
      "(Epoch 0 / 2) train acc: 0.156000; val_acc: 0.156000\n",
      "(Iteration 21 / 1960) loss: 2.121726\n",
      "(Iteration 41 / 1960) loss: 2.103560\n",
      "(Iteration 61 / 1960) loss: 1.957840\n",
      "(Iteration 81 / 1960) loss: 1.916217\n",
      "(Iteration 101 / 1960) loss: 1.862944\n",
      "(Iteration 121 / 1960) loss: 1.823076\n",
      "(Iteration 141 / 1960) loss: 1.770415\n",
      "(Iteration 161 / 1960) loss: 1.475102\n",
      "(Iteration 181 / 1960) loss: 1.589529\n",
      "(Iteration 201 / 1960) loss: 1.506223\n",
      "(Iteration 221 / 1960) loss: 1.676077\n",
      "(Iteration 241 / 1960) loss: 1.907258\n",
      "(Iteration 261 / 1960) loss: 1.462066\n",
      "(Iteration 281 / 1960) loss: 1.712182\n",
      "(Iteration 301 / 1960) loss: 1.664856\n",
      "(Iteration 321 / 1960) loss: 1.606155\n",
      "(Iteration 341 / 1960) loss: 1.356205\n",
      "(Iteration 361 / 1960) loss: 1.394954\n",
      "(Iteration 381 / 1960) loss: 1.379786\n",
      "(Iteration 401 / 1960) loss: 1.375081\n",
      "(Iteration 421 / 1960) loss: 1.529978\n",
      "(Iteration 441 / 1960) loss: 1.722161\n",
      "(Iteration 461 / 1960) loss: 1.346307\n",
      "(Iteration 481 / 1960) loss: 1.633570\n",
      "(Iteration 501 / 1960) loss: 1.423436\n",
      "(Iteration 521 / 1960) loss: 1.351966\n",
      "(Iteration 541 / 1960) loss: 1.687844\n",
      "(Iteration 561 / 1960) loss: 1.123380\n",
      "(Iteration 581 / 1960) loss: 1.583241\n",
      "(Iteration 601 / 1960) loss: 1.201828\n",
      "(Iteration 621 / 1960) loss: 1.311307\n",
      "(Iteration 641 / 1960) loss: 1.693205\n",
      "(Iteration 661 / 1960) loss: 1.285638\n",
      "(Iteration 681 / 1960) loss: 1.631181\n",
      "(Iteration 701 / 1960) loss: 1.309173\n",
      "(Iteration 721 / 1960) loss: 1.451794\n",
      "(Iteration 741 / 1960) loss: 1.250900\n",
      "(Iteration 761 / 1960) loss: 1.176354\n",
      "(Iteration 781 / 1960) loss: 1.424711\n",
      "(Iteration 801 / 1960) loss: 1.470753\n",
      "(Iteration 821 / 1960) loss: 1.372470\n",
      "(Iteration 841 / 1960) loss: 1.214976\n",
      "(Iteration 861 / 1960) loss: 1.199374\n",
      "(Iteration 881 / 1960) loss: 1.149966\n",
      "(Iteration 901 / 1960) loss: 0.892652\n",
      "(Iteration 921 / 1960) loss: 1.176601\n",
      "(Iteration 941 / 1960) loss: 1.323326\n",
      "(Iteration 961 / 1960) loss: 1.129080\n",
      "(Epoch 1 / 2) train acc: 0.540000; val_acc: 0.551000\n",
      "(Iteration 981 / 1960) loss: 1.239132\n",
      "(Iteration 1001 / 1960) loss: 1.396851\n",
      "(Iteration 1021 / 1960) loss: 1.328875\n",
      "(Iteration 1041 / 1960) loss: 1.199434\n",
      "(Iteration 1061 / 1960) loss: 1.010824\n",
      "(Iteration 1081 / 1960) loss: 1.138259\n",
      "(Iteration 1101 / 1960) loss: 1.315947\n",
      "(Iteration 1121 / 1960) loss: 1.214666\n",
      "(Iteration 1141 / 1960) loss: 1.212406\n",
      "(Iteration 1161 / 1960) loss: 1.363444\n",
      "(Iteration 1181 / 1960) loss: 1.125720\n",
      "(Iteration 1201 / 1960) loss: 1.559527\n",
      "(Iteration 1221 / 1960) loss: 1.457267\n",
      "(Iteration 1241 / 1960) loss: 1.253429\n",
      "(Iteration 1261 / 1960) loss: 1.098994\n",
      "(Iteration 1281 / 1960) loss: 0.998006\n",
      "(Iteration 1301 / 1960) loss: 1.142996\n",
      "(Iteration 1321 / 1960) loss: 1.203789\n",
      "(Iteration 1341 / 1960) loss: 1.192234\n",
      "(Iteration 1361 / 1960) loss: 1.329194\n",
      "(Iteration 1381 / 1960) loss: 1.363107\n",
      "(Iteration 1401 / 1960) loss: 1.019178\n",
      "(Iteration 1421 / 1960) loss: 1.228175\n",
      "(Iteration 1441 / 1960) loss: 1.007542\n",
      "(Iteration 1461 / 1960) loss: 1.156554\n",
      "(Iteration 1481 / 1960) loss: 1.022607\n",
      "(Iteration 1501 / 1960) loss: 1.021290\n",
      "(Iteration 1521 / 1960) loss: 1.122882\n",
      "(Iteration 1541 / 1960) loss: 1.104948\n",
      "(Iteration 1561 / 1960) loss: 1.193317\n",
      "(Iteration 1581 / 1960) loss: 1.046477\n",
      "(Iteration 1601 / 1960) loss: 1.286421\n",
      "(Iteration 1621 / 1960) loss: 1.070511\n",
      "(Iteration 1641 / 1960) loss: 1.147913\n",
      "(Iteration 1661 / 1960) loss: 1.087897\n",
      "(Iteration 1681 / 1960) loss: 1.086168\n",
      "(Iteration 1701 / 1960) loss: 1.025387\n",
      "(Iteration 1721 / 1960) loss: 1.239860\n",
      "(Iteration 1741 / 1960) loss: 1.058594\n",
      "(Iteration 1761 / 1960) loss: 1.082230\n",
      "(Iteration 1781 / 1960) loss: 1.213108\n",
      "(Iteration 1801 / 1960) loss: 1.142709\n",
      "(Iteration 1821 / 1960) loss: 0.931750\n",
      "(Iteration 1841 / 1960) loss: 1.042478\n",
      "(Iteration 1861 / 1960) loss: 1.105257\n",
      "(Iteration 1881 / 1960) loss: 0.953663\n",
      "(Iteration 1901 / 1960) loss: 1.085788\n",
      "(Iteration 1921 / 1960) loss: 1.031605\n",
      "(Iteration 1941 / 1960) loss: 1.252454\n",
      "(Epoch 2 / 2) train acc: 0.658000; val_acc: 0.588000\n",
      "final accuracy for 100 percent of data, subspace model: 0.6320 (train), 0.6040 (test)\n",
      "----- trial 1: 80 percent of data (39200 unique examples, 9800 duplicate examples) -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING STANDARD MODEL FOR TRIAL 1 ===\n",
      "(Iteration 1 / 1960) loss: 2.304943\n",
      "(Epoch 0 / 2) train acc: 0.151000; val_acc: 0.158000\n",
      "(Iteration 21 / 1960) loss: 2.117028\n",
      "(Iteration 41 / 1960) loss: 1.832338\n",
      "(Iteration 61 / 1960) loss: 1.805229\n",
      "(Iteration 81 / 1960) loss: 2.039425\n",
      "(Iteration 101 / 1960) loss: 1.548470\n",
      "(Iteration 121 / 1960) loss: 1.731984\n",
      "(Iteration 141 / 1960) loss: 1.800997\n",
      "(Iteration 161 / 1960) loss: 1.344682\n",
      "(Iteration 181 / 1960) loss: 1.206739\n",
      "(Iteration 201 / 1960) loss: 1.771347\n",
      "(Iteration 221 / 1960) loss: 1.590330\n",
      "(Iteration 241 / 1960) loss: 1.683910\n",
      "(Iteration 261 / 1960) loss: 1.568958\n",
      "(Iteration 281 / 1960) loss: 1.462315\n",
      "(Iteration 301 / 1960) loss: 1.626151\n",
      "(Iteration 321 / 1960) loss: 1.304370\n",
      "(Iteration 341 / 1960) loss: 1.684724\n",
      "(Iteration 361 / 1960) loss: 1.423770\n",
      "(Iteration 381 / 1960) loss: 1.623692\n",
      "(Iteration 401 / 1960) loss: 1.715405\n",
      "(Iteration 421 / 1960) loss: 1.630377\n",
      "(Iteration 441 / 1960) loss: 1.511309\n",
      "(Iteration 461 / 1960) loss: 1.449063\n",
      "(Iteration 481 / 1960) loss: 1.360597\n",
      "(Iteration 501 / 1960) loss: 1.051263\n",
      "(Iteration 521 / 1960) loss: 1.340731\n",
      "(Iteration 541 / 1960) loss: 1.496125\n",
      "(Iteration 561 / 1960) loss: 1.585928\n",
      "(Iteration 581 / 1960) loss: 1.419024\n",
      "(Iteration 601 / 1960) loss: 1.550924\n",
      "(Iteration 621 / 1960) loss: 1.355241\n",
      "(Iteration 641 / 1960) loss: 1.208545\n",
      "(Iteration 661 / 1960) loss: 1.332842\n",
      "(Iteration 681 / 1960) loss: 1.243808\n",
      "(Iteration 701 / 1960) loss: 1.088425\n",
      "(Iteration 721 / 1960) loss: 1.158793\n",
      "(Iteration 741 / 1960) loss: 1.537115\n",
      "(Iteration 761 / 1960) loss: 1.150061\n",
      "(Iteration 781 / 1960) loss: 0.925908\n",
      "(Iteration 801 / 1960) loss: 1.336645\n",
      "(Iteration 821 / 1960) loss: 1.223520\n",
      "(Iteration 841 / 1960) loss: 1.091159\n",
      "(Iteration 861 / 1960) loss: 1.190350\n",
      "(Iteration 881 / 1960) loss: 1.551909\n",
      "(Iteration 901 / 1960) loss: 1.018304\n",
      "(Iteration 921 / 1960) loss: 1.267717\n",
      "(Iteration 941 / 1960) loss: 1.327045\n",
      "(Iteration 961 / 1960) loss: 1.334456\n",
      "(Epoch 1 / 2) train acc: 0.554000; val_acc: 0.568000\n",
      "(Iteration 981 / 1960) loss: 1.270583\n",
      "(Iteration 1001 / 1960) loss: 1.087661\n",
      "(Iteration 1021 / 1960) loss: 1.172386\n",
      "(Iteration 1041 / 1960) loss: 1.232252\n",
      "(Iteration 1061 / 1960) loss: 0.806418\n",
      "(Iteration 1081 / 1960) loss: 1.083761\n",
      "(Iteration 1101 / 1960) loss: 1.353341\n",
      "(Iteration 1121 / 1960) loss: 1.382161\n",
      "(Iteration 1141 / 1960) loss: 0.993850\n",
      "(Iteration 1161 / 1960) loss: 1.111986\n",
      "(Iteration 1181 / 1960) loss: 1.387031\n",
      "(Iteration 1201 / 1960) loss: 1.274028\n",
      "(Iteration 1221 / 1960) loss: 1.412922\n",
      "(Iteration 1241 / 1960) loss: 1.434849\n",
      "(Iteration 1261 / 1960) loss: 0.879505\n",
      "(Iteration 1281 / 1960) loss: 1.522057\n",
      "(Iteration 1301 / 1960) loss: 1.153582\n",
      "(Iteration 1321 / 1960) loss: 1.192125\n",
      "(Iteration 1341 / 1960) loss: 1.076488\n",
      "(Iteration 1361 / 1960) loss: 1.094600\n",
      "(Iteration 1381 / 1960) loss: 1.037158\n",
      "(Iteration 1401 / 1960) loss: 0.831144\n",
      "(Iteration 1421 / 1960) loss: 0.867825\n",
      "(Iteration 1441 / 1960) loss: 1.114457\n",
      "(Iteration 1461 / 1960) loss: 1.219475\n",
      "(Iteration 1481 / 1960) loss: 1.213217\n",
      "(Iteration 1501 / 1960) loss: 1.180815\n",
      "(Iteration 1521 / 1960) loss: 0.860666\n",
      "(Iteration 1541 / 1960) loss: 1.183793\n",
      "(Iteration 1561 / 1960) loss: 0.968674\n",
      "(Iteration 1581 / 1960) loss: 0.901954\n",
      "(Iteration 1601 / 1960) loss: 0.971937\n",
      "(Iteration 1621 / 1960) loss: 1.058192\n",
      "(Iteration 1641 / 1960) loss: 1.132803\n",
      "(Iteration 1661 / 1960) loss: 1.020500\n",
      "(Iteration 1681 / 1960) loss: 1.035060\n",
      "(Iteration 1701 / 1960) loss: 0.862952\n",
      "(Iteration 1721 / 1960) loss: 1.216936\n",
      "(Iteration 1741 / 1960) loss: 0.929315\n",
      "(Iteration 1761 / 1960) loss: 1.092424\n",
      "(Iteration 1781 / 1960) loss: 0.828474\n",
      "(Iteration 1801 / 1960) loss: 1.013033\n",
      "(Iteration 1821 / 1960) loss: 0.880287\n",
      "(Iteration 1841 / 1960) loss: 1.112794\n",
      "(Iteration 1861 / 1960) loss: 0.827956\n",
      "(Iteration 1881 / 1960) loss: 0.958836\n",
      "(Iteration 1901 / 1960) loss: 0.878713\n",
      "(Iteration 1921 / 1960) loss: 0.660798\n",
      "(Iteration 1941 / 1960) loss: 1.094821\n",
      "(Epoch 2 / 2) train acc: 0.692000; val_acc: 0.587000\n",
      "final accuracy for 80 percent of data, standard model: 0.6920 (train), 0.0000 (test)\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 1 ===\n",
      "(Iteration 1 / 1960) loss: 2.305238\n",
      "(Epoch 0 / 2) train acc: 0.152000; val_acc: 0.146000\n",
      "(Iteration 21 / 1960) loss: 1.983802\n",
      "(Iteration 41 / 1960) loss: 1.922825\n",
      "(Iteration 61 / 1960) loss: 1.887187\n",
      "(Iteration 81 / 1960) loss: 1.722024\n",
      "(Iteration 101 / 1960) loss: 1.662441\n",
      "(Iteration 121 / 1960) loss: 1.599176\n",
      "(Iteration 141 / 1960) loss: 1.564438\n",
      "(Iteration 161 / 1960) loss: 1.623921\n",
      "(Iteration 181 / 1960) loss: 1.709887\n",
      "(Iteration 201 / 1960) loss: 1.753376\n",
      "(Iteration 221 / 1960) loss: 1.571581\n",
      "(Iteration 241 / 1960) loss: 1.615290\n",
      "(Iteration 261 / 1960) loss: 1.747597\n",
      "(Iteration 281 / 1960) loss: 1.438129\n",
      "(Iteration 301 / 1960) loss: 1.632917\n",
      "(Iteration 321 / 1960) loss: 1.397306\n",
      "(Iteration 341 / 1960) loss: 1.591966\n",
      "(Iteration 361 / 1960) loss: 1.576971\n",
      "(Iteration 381 / 1960) loss: 1.647015\n",
      "(Iteration 401 / 1960) loss: 1.174194\n",
      "(Iteration 421 / 1960) loss: 1.278524\n",
      "(Iteration 441 / 1960) loss: 1.692505\n",
      "(Iteration 461 / 1960) loss: 1.608510\n",
      "(Iteration 481 / 1960) loss: 1.279771\n",
      "(Iteration 501 / 1960) loss: 1.418305\n",
      "(Iteration 521 / 1960) loss: 1.549528\n",
      "(Iteration 541 / 1960) loss: 1.481694\n",
      "(Iteration 561 / 1960) loss: 1.496860\n",
      "(Iteration 581 / 1960) loss: 1.626383\n",
      "(Iteration 601 / 1960) loss: 1.221651\n",
      "(Iteration 621 / 1960) loss: 1.367488\n",
      "(Iteration 641 / 1960) loss: 1.523007\n",
      "(Iteration 661 / 1960) loss: 1.290461\n",
      "(Iteration 681 / 1960) loss: 1.260865\n",
      "(Iteration 701 / 1960) loss: 1.247967\n",
      "(Iteration 721 / 1960) loss: 1.562782\n",
      "(Iteration 741 / 1960) loss: 1.174335\n",
      "(Iteration 761 / 1960) loss: 1.252477\n",
      "(Iteration 781 / 1960) loss: 1.033281\n",
      "(Iteration 801 / 1960) loss: 1.180982\n",
      "(Iteration 821 / 1960) loss: 1.395246\n",
      "(Iteration 841 / 1960) loss: 1.549361\n",
      "(Iteration 861 / 1960) loss: 1.090547\n",
      "(Iteration 881 / 1960) loss: 1.296184\n",
      "(Iteration 901 / 1960) loss: 1.332223\n",
      "(Iteration 921 / 1960) loss: 1.116786\n",
      "(Iteration 941 / 1960) loss: 1.309331\n",
      "(Iteration 961 / 1960) loss: 1.216138\n",
      "(Epoch 1 / 2) train acc: 0.567000; val_acc: 0.554000\n",
      "(Iteration 981 / 1960) loss: 1.054861\n",
      "(Iteration 1001 / 1960) loss: 1.104178\n",
      "(Iteration 1021 / 1960) loss: 1.052064\n",
      "(Iteration 1041 / 1960) loss: 1.225642\n",
      "(Iteration 1061 / 1960) loss: 1.337849\n",
      "(Iteration 1081 / 1960) loss: 1.242640\n",
      "(Iteration 1101 / 1960) loss: 1.082546\n",
      "(Iteration 1121 / 1960) loss: 1.269765\n",
      "(Iteration 1141 / 1960) loss: 1.209497\n",
      "(Iteration 1161 / 1960) loss: 1.249303\n",
      "(Iteration 1181 / 1960) loss: 1.119508\n",
      "(Iteration 1201 / 1960) loss: 1.155269\n",
      "(Iteration 1221 / 1960) loss: 0.950605\n",
      "(Iteration 1241 / 1960) loss: 1.044458\n",
      "(Iteration 1261 / 1960) loss: 1.140327\n",
      "(Iteration 1281 / 1960) loss: 1.344894\n",
      "(Iteration 1301 / 1960) loss: 1.056803\n",
      "(Iteration 1321 / 1960) loss: 1.127298\n",
      "(Iteration 1341 / 1960) loss: 1.487826\n",
      "(Iteration 1361 / 1960) loss: 0.987141\n",
      "(Iteration 1381 / 1960) loss: 1.086122\n",
      "(Iteration 1401 / 1960) loss: 1.119948\n",
      "(Iteration 1421 / 1960) loss: 1.148907\n",
      "(Iteration 1441 / 1960) loss: 1.139347\n",
      "(Iteration 1461 / 1960) loss: 1.264285\n",
      "(Iteration 1481 / 1960) loss: 1.036587\n",
      "(Iteration 1501 / 1960) loss: 1.114240\n",
      "(Iteration 1521 / 1960) loss: 1.202896\n",
      "(Iteration 1541 / 1960) loss: 1.113654\n",
      "(Iteration 1561 / 1960) loss: 1.001331\n",
      "(Iteration 1581 / 1960) loss: 1.147964\n",
      "(Iteration 1601 / 1960) loss: 1.268995\n",
      "(Iteration 1621 / 1960) loss: 1.340382\n",
      "(Iteration 1641 / 1960) loss: 1.253765\n",
      "(Iteration 1661 / 1960) loss: 0.933631\n",
      "(Iteration 1681 / 1960) loss: 1.181064\n",
      "(Iteration 1701 / 1960) loss: 1.233110\n",
      "(Iteration 1721 / 1960) loss: 1.045512\n",
      "(Iteration 1741 / 1960) loss: 1.037940\n",
      "(Iteration 1761 / 1960) loss: 1.260491\n",
      "(Iteration 1781 / 1960) loss: 0.933325\n",
      "(Iteration 1801 / 1960) loss: 1.201895\n",
      "(Iteration 1821 / 1960) loss: 1.115417\n",
      "(Iteration 1841 / 1960) loss: 0.978717\n",
      "(Iteration 1861 / 1960) loss: 1.151760\n",
      "(Iteration 1881 / 1960) loss: 0.868269\n",
      "(Iteration 1901 / 1960) loss: 0.784429\n",
      "(Iteration 1921 / 1960) loss: 1.159419\n",
      "(Iteration 1941 / 1960) loss: 1.024000\n",
      "(Epoch 2 / 2) train acc: 0.699000; val_acc: 0.609000\n",
      "final accuracy for 80 percent of data, subspace model: 0.6920 (train), 0.6030 (test)\n",
      "----- trial 2: 60 percent of data (29400 unique examples, 19600 duplicate examples) -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING STANDARD MODEL FOR TRIAL 2 ===\n",
      "(Iteration 1 / 1960) loss: 2.304949\n",
      "(Epoch 0 / 2) train acc: 0.120000; val_acc: 0.124000\n",
      "(Iteration 21 / 1960) loss: 2.049667\n",
      "(Iteration 41 / 1960) loss: 1.983096\n",
      "(Iteration 61 / 1960) loss: 1.834232\n",
      "(Iteration 81 / 1960) loss: 1.993941\n",
      "(Iteration 101 / 1960) loss: 1.456182\n",
      "(Iteration 121 / 1960) loss: 1.850919\n",
      "(Iteration 141 / 1960) loss: 1.316464\n",
      "(Iteration 161 / 1960) loss: 1.665854\n",
      "(Iteration 181 / 1960) loss: 1.553799\n",
      "(Iteration 201 / 1960) loss: 1.434514\n",
      "(Iteration 221 / 1960) loss: 1.340702\n",
      "(Iteration 241 / 1960) loss: 1.555840\n",
      "(Iteration 261 / 1960) loss: 1.695310\n",
      "(Iteration 281 / 1960) loss: 1.654336\n",
      "(Iteration 301 / 1960) loss: 1.675273\n",
      "(Iteration 321 / 1960) loss: 1.497057\n",
      "(Iteration 341 / 1960) loss: 1.548534\n",
      "(Iteration 361 / 1960) loss: 1.335467\n",
      "(Iteration 381 / 1960) loss: 1.540241\n",
      "(Iteration 401 / 1960) loss: 1.310434\n",
      "(Iteration 421 / 1960) loss: 1.303692\n",
      "(Iteration 441 / 1960) loss: 1.328130\n",
      "(Iteration 461 / 1960) loss: 1.226108\n",
      "(Iteration 481 / 1960) loss: 1.186120\n",
      "(Iteration 501 / 1960) loss: 1.051171\n",
      "(Iteration 521 / 1960) loss: 1.425458\n",
      "(Iteration 541 / 1960) loss: 1.490877\n",
      "(Iteration 561 / 1960) loss: 1.805142\n",
      "(Iteration 581 / 1960) loss: 1.403667\n",
      "(Iteration 601 / 1960) loss: 1.177577\n",
      "(Iteration 621 / 1960) loss: 1.062424\n",
      "(Iteration 641 / 1960) loss: 1.348873\n",
      "(Iteration 661 / 1960) loss: 1.420818\n",
      "(Iteration 681 / 1960) loss: 1.486011\n",
      "(Iteration 701 / 1960) loss: 1.089252\n",
      "(Iteration 721 / 1960) loss: 1.150380\n",
      "(Iteration 741 / 1960) loss: 1.268089\n",
      "(Iteration 761 / 1960) loss: 1.330283\n",
      "(Iteration 781 / 1960) loss: 1.388585\n",
      "(Iteration 801 / 1960) loss: 1.295778\n",
      "(Iteration 821 / 1960) loss: 1.040107\n",
      "(Iteration 841 / 1960) loss: 1.262696\n",
      "(Iteration 861 / 1960) loss: 1.274300\n",
      "(Iteration 881 / 1960) loss: 1.129864\n",
      "(Iteration 901 / 1960) loss: 1.013740\n",
      "(Iteration 921 / 1960) loss: 1.420436\n",
      "(Iteration 941 / 1960) loss: 1.054636\n",
      "(Iteration 961 / 1960) loss: 0.837076\n",
      "(Epoch 1 / 2) train acc: 0.573000; val_acc: 0.543000\n",
      "(Iteration 981 / 1960) loss: 1.485287\n",
      "(Iteration 1001 / 1960) loss: 1.143097\n",
      "(Iteration 1021 / 1960) loss: 1.148143\n",
      "(Iteration 1041 / 1960) loss: 1.147745\n",
      "(Iteration 1061 / 1960) loss: 1.082256\n",
      "(Iteration 1081 / 1960) loss: 0.946499\n",
      "(Iteration 1101 / 1960) loss: 1.579236\n",
      "(Iteration 1121 / 1960) loss: 1.373177\n",
      "(Iteration 1141 / 1960) loss: 1.002661\n",
      "(Iteration 1161 / 1960) loss: 1.110462\n",
      "(Iteration 1181 / 1960) loss: 0.730772\n",
      "(Iteration 1201 / 1960) loss: 1.032444\n",
      "(Iteration 1221 / 1960) loss: 1.257069\n",
      "(Iteration 1241 / 1960) loss: 0.930448\n",
      "(Iteration 1261 / 1960) loss: 1.102763\n",
      "(Iteration 1281 / 1960) loss: 1.096143\n",
      "(Iteration 1301 / 1960) loss: 1.075398\n",
      "(Iteration 1321 / 1960) loss: 1.177614\n",
      "(Iteration 1341 / 1960) loss: 1.111465\n",
      "(Iteration 1361 / 1960) loss: 0.965216\n",
      "(Iteration 1381 / 1960) loss: 1.010403\n",
      "(Iteration 1401 / 1960) loss: 1.358489\n",
      "(Iteration 1421 / 1960) loss: 1.019176\n",
      "(Iteration 1441 / 1960) loss: 0.906610\n",
      "(Iteration 1461 / 1960) loss: 0.974579\n",
      "(Iteration 1481 / 1960) loss: 1.158758\n",
      "(Iteration 1501 / 1960) loss: 0.989089\n",
      "(Iteration 1521 / 1960) loss: 1.051073\n",
      "(Iteration 1541 / 1960) loss: 0.730437\n",
      "(Iteration 1561 / 1960) loss: 0.928247\n",
      "(Iteration 1581 / 1960) loss: 1.023749\n",
      "(Iteration 1601 / 1960) loss: 0.805906\n",
      "(Iteration 1621 / 1960) loss: 0.683490\n",
      "(Iteration 1641 / 1960) loss: 0.761212\n",
      "(Iteration 1661 / 1960) loss: 1.236592\n",
      "(Iteration 1681 / 1960) loss: 1.079779\n",
      "(Iteration 1701 / 1960) loss: 0.655080\n",
      "(Iteration 1721 / 1960) loss: 0.840719\n",
      "(Iteration 1741 / 1960) loss: 1.129384\n",
      "(Iteration 1761 / 1960) loss: 0.861006\n",
      "(Iteration 1781 / 1960) loss: 1.014015\n",
      "(Iteration 1801 / 1960) loss: 0.964319\n",
      "(Iteration 1821 / 1960) loss: 0.753385\n",
      "(Iteration 1841 / 1960) loss: 0.922277\n",
      "(Iteration 1861 / 1960) loss: 0.908977\n",
      "(Iteration 1881 / 1960) loss: 0.932852\n",
      "(Iteration 1901 / 1960) loss: 0.573481\n",
      "(Iteration 1921 / 1960) loss: 0.815740\n",
      "(Iteration 1941 / 1960) loss: 0.935589\n",
      "(Epoch 2 / 2) train acc: 0.715000; val_acc: 0.597000\n",
      "final accuracy for 60 percent of data, standard model: 0.7150 (train), 0.0000 (test)\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 2 ===\n",
      "(Iteration 1 / 1960) loss: 2.304827\n",
      "(Epoch 0 / 2) train acc: 0.190000; val_acc: 0.195000\n",
      "(Iteration 21 / 1960) loss: 2.255183\n",
      "(Iteration 41 / 1960) loss: 2.222222\n",
      "(Iteration 61 / 1960) loss: 1.846394\n",
      "(Iteration 81 / 1960) loss: 1.748769\n",
      "(Iteration 101 / 1960) loss: 1.737121\n",
      "(Iteration 121 / 1960) loss: 1.935330\n",
      "(Iteration 141 / 1960) loss: 1.934174\n",
      "(Iteration 161 / 1960) loss: 1.604431\n",
      "(Iteration 181 / 1960) loss: 1.757710\n",
      "(Iteration 201 / 1960) loss: 1.685369\n",
      "(Iteration 221 / 1960) loss: 1.755694\n",
      "(Iteration 241 / 1960) loss: 1.556872\n",
      "(Iteration 261 / 1960) loss: 1.787420\n",
      "(Iteration 281 / 1960) loss: 1.504945\n",
      "(Iteration 301 / 1960) loss: 1.522444\n",
      "(Iteration 321 / 1960) loss: 1.356013\n",
      "(Iteration 341 / 1960) loss: 1.720710\n",
      "(Iteration 361 / 1960) loss: 1.362735\n",
      "(Iteration 381 / 1960) loss: 1.510812\n",
      "(Iteration 401 / 1960) loss: 1.430763\n",
      "(Iteration 421 / 1960) loss: 1.309397\n",
      "(Iteration 441 / 1960) loss: 1.145070\n",
      "(Iteration 461 / 1960) loss: 1.451947\n",
      "(Iteration 481 / 1960) loss: 1.112700\n",
      "(Iteration 501 / 1960) loss: 1.231939\n",
      "(Iteration 521 / 1960) loss: 1.347384\n",
      "(Iteration 541 / 1960) loss: 1.797184\n",
      "(Iteration 561 / 1960) loss: 1.358993\n",
      "(Iteration 581 / 1960) loss: 1.122930\n",
      "(Iteration 601 / 1960) loss: 1.238317\n",
      "(Iteration 621 / 1960) loss: 1.250926\n",
      "(Iteration 641 / 1960) loss: 1.407774\n",
      "(Iteration 661 / 1960) loss: 1.234716\n",
      "(Iteration 681 / 1960) loss: 1.490126\n",
      "(Iteration 701 / 1960) loss: 1.274266\n",
      "(Iteration 721 / 1960) loss: 1.510580\n",
      "(Iteration 741 / 1960) loss: 1.401756\n",
      "(Iteration 761 / 1960) loss: 1.075891\n",
      "(Iteration 781 / 1960) loss: 1.252300\n",
      "(Iteration 801 / 1960) loss: 1.309276\n",
      "(Iteration 821 / 1960) loss: 1.590359\n",
      "(Iteration 841 / 1960) loss: 1.537481\n",
      "(Iteration 861 / 1960) loss: 1.342785\n",
      "(Iteration 881 / 1960) loss: 1.149617\n",
      "(Iteration 901 / 1960) loss: 1.106248\n",
      "(Iteration 921 / 1960) loss: 1.510827\n",
      "(Iteration 941 / 1960) loss: 1.031027\n",
      "(Iteration 961 / 1960) loss: 1.301632\n",
      "(Epoch 1 / 2) train acc: 0.575000; val_acc: 0.543000\n",
      "(Iteration 981 / 1960) loss: 1.495292\n",
      "(Iteration 1001 / 1960) loss: 0.971798\n",
      "(Iteration 1021 / 1960) loss: 1.026440\n",
      "(Iteration 1041 / 1960) loss: 1.063709\n",
      "(Iteration 1061 / 1960) loss: 1.145933\n",
      "(Iteration 1081 / 1960) loss: 1.029375\n",
      "(Iteration 1101 / 1960) loss: 1.222339\n",
      "(Iteration 1121 / 1960) loss: 1.041709\n",
      "(Iteration 1141 / 1960) loss: 1.102483\n",
      "(Iteration 1161 / 1960) loss: 1.270431\n",
      "(Iteration 1181 / 1960) loss: 1.114571\n",
      "(Iteration 1201 / 1960) loss: 1.145124\n",
      "(Iteration 1221 / 1960) loss: 1.207268\n",
      "(Iteration 1241 / 1960) loss: 0.869709\n",
      "(Iteration 1261 / 1960) loss: 1.006959\n",
      "(Iteration 1281 / 1960) loss: 0.973346\n",
      "(Iteration 1301 / 1960) loss: 0.971540\n",
      "(Iteration 1321 / 1960) loss: 0.988711\n",
      "(Iteration 1341 / 1960) loss: 1.354960\n",
      "(Iteration 1361 / 1960) loss: 0.970448\n",
      "(Iteration 1381 / 1960) loss: 1.037571\n",
      "(Iteration 1401 / 1960) loss: 1.137822\n",
      "(Iteration 1421 / 1960) loss: 1.016767\n",
      "(Iteration 1441 / 1960) loss: 1.047691\n",
      "(Iteration 1461 / 1960) loss: 0.950999\n",
      "(Iteration 1481 / 1960) loss: 1.283788\n",
      "(Iteration 1501 / 1960) loss: 1.172459\n",
      "(Iteration 1521 / 1960) loss: 1.000754\n",
      "(Iteration 1541 / 1960) loss: 0.704127\n",
      "(Iteration 1561 / 1960) loss: 1.102648\n",
      "(Iteration 1581 / 1960) loss: 1.025193\n",
      "(Iteration 1601 / 1960) loss: 0.947334\n",
      "(Iteration 1621 / 1960) loss: 1.467220\n",
      "(Iteration 1641 / 1960) loss: 0.793060\n",
      "(Iteration 1661 / 1960) loss: 0.880996\n",
      "(Iteration 1681 / 1960) loss: 1.037588\n",
      "(Iteration 1701 / 1960) loss: 0.763566\n",
      "(Iteration 1721 / 1960) loss: 0.934475\n",
      "(Iteration 1741 / 1960) loss: 0.775919\n",
      "(Iteration 1761 / 1960) loss: 1.033042\n",
      "(Iteration 1781 / 1960) loss: 0.728844\n",
      "(Iteration 1801 / 1960) loss: 1.273702\n",
      "(Iteration 1821 / 1960) loss: 0.925919\n",
      "(Iteration 1841 / 1960) loss: 0.918015\n",
      "(Iteration 1861 / 1960) loss: 1.005786\n",
      "(Iteration 1881 / 1960) loss: 0.870171\n",
      "(Iteration 1901 / 1960) loss: 0.788350\n",
      "(Iteration 1921 / 1960) loss: 0.838676\n",
      "(Iteration 1941 / 1960) loss: 0.701290\n",
      "(Epoch 2 / 2) train acc: 0.701000; val_acc: 0.584000\n",
      "final accuracy for 60 percent of data, subspace model: 0.7150 (train), 0.5730 (test)\n",
      "----- trial 3: 40 percent of data (19600 unique examples, 29400 duplicate examples) -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING STANDARD MODEL FOR TRIAL 3 ===\n",
      "(Iteration 1 / 1960) loss: 2.304651\n",
      "(Epoch 0 / 2) train acc: 0.176000; val_acc: 0.148000\n",
      "(Iteration 21 / 1960) loss: 2.137957\n",
      "(Iteration 41 / 1960) loss: 1.799657\n",
      "(Iteration 61 / 1960) loss: 2.065138\n",
      "(Iteration 81 / 1960) loss: 1.663573\n",
      "(Iteration 101 / 1960) loss: 1.501495\n",
      "(Iteration 121 / 1960) loss: 1.724843\n",
      "(Iteration 141 / 1960) loss: 1.941786\n",
      "(Iteration 161 / 1960) loss: 1.628663\n",
      "(Iteration 181 / 1960) loss: 1.535586\n",
      "(Iteration 201 / 1960) loss: 1.636595\n",
      "(Iteration 221 / 1960) loss: 1.558481\n",
      "(Iteration 241 / 1960) loss: 1.738307\n",
      "(Iteration 261 / 1960) loss: 1.363172\n",
      "(Iteration 281 / 1960) loss: 1.202737\n",
      "(Iteration 301 / 1960) loss: 1.517403\n",
      "(Iteration 321 / 1960) loss: 1.296008\n",
      "(Iteration 341 / 1960) loss: 1.602054\n",
      "(Iteration 361 / 1960) loss: 1.394601\n",
      "(Iteration 381 / 1960) loss: 1.271911\n",
      "(Iteration 401 / 1960) loss: 1.099083\n",
      "(Iteration 421 / 1960) loss: 1.292488\n",
      "(Iteration 441 / 1960) loss: 1.402308\n",
      "(Iteration 461 / 1960) loss: 1.515910\n",
      "(Iteration 481 / 1960) loss: 1.409639\n",
      "(Iteration 501 / 1960) loss: 1.785905\n",
      "(Iteration 521 / 1960) loss: 1.208163\n",
      "(Iteration 541 / 1960) loss: 1.487961\n",
      "(Iteration 561 / 1960) loss: 1.306443\n",
      "(Iteration 581 / 1960) loss: 1.556614\n",
      "(Iteration 601 / 1960) loss: 1.190876\n",
      "(Iteration 621 / 1960) loss: 1.141349\n",
      "(Iteration 641 / 1960) loss: 1.224383\n",
      "(Iteration 661 / 1960) loss: 1.118889\n",
      "(Iteration 681 / 1960) loss: 1.426863\n",
      "(Iteration 701 / 1960) loss: 1.136568\n",
      "(Iteration 721 / 1960) loss: 1.141867\n",
      "(Iteration 741 / 1960) loss: 1.281173\n",
      "(Iteration 761 / 1960) loss: 1.211680\n",
      "(Iteration 781 / 1960) loss: 1.079144\n",
      "(Iteration 801 / 1960) loss: 0.881656\n",
      "(Iteration 821 / 1960) loss: 1.050136\n",
      "(Iteration 841 / 1960) loss: 1.313673\n",
      "(Iteration 861 / 1960) loss: 1.276314\n",
      "(Iteration 881 / 1960) loss: 1.309533\n",
      "(Iteration 901 / 1960) loss: 1.622979\n",
      "(Iteration 921 / 1960) loss: 1.156286\n",
      "(Iteration 941 / 1960) loss: 1.014984\n",
      "(Iteration 961 / 1960) loss: 1.232949\n",
      "(Epoch 1 / 2) train acc: 0.618000; val_acc: 0.542000\n",
      "(Iteration 981 / 1960) loss: 1.021541\n",
      "(Iteration 1001 / 1960) loss: 0.931829\n",
      "(Iteration 1021 / 1960) loss: 1.148628\n",
      "(Iteration 1041 / 1960) loss: 1.127479\n",
      "(Iteration 1061 / 1960) loss: 1.152152\n",
      "(Iteration 1081 / 1960) loss: 1.119875\n",
      "(Iteration 1101 / 1960) loss: 1.112752\n",
      "(Iteration 1121 / 1960) loss: 0.855025\n",
      "(Iteration 1141 / 1960) loss: 0.879261\n",
      "(Iteration 1161 / 1960) loss: 1.058461\n",
      "(Iteration 1181 / 1960) loss: 0.830748\n",
      "(Iteration 1201 / 1960) loss: 1.027024\n",
      "(Iteration 1221 / 1960) loss: 1.426396\n",
      "(Iteration 1241 / 1960) loss: 0.906130\n",
      "(Iteration 1261 / 1960) loss: 0.928613\n",
      "(Iteration 1281 / 1960) loss: 0.605556\n",
      "(Iteration 1301 / 1960) loss: 0.848855\n",
      "(Iteration 1321 / 1960) loss: 0.979999\n",
      "(Iteration 1341 / 1960) loss: 1.052123\n",
      "(Iteration 1361 / 1960) loss: 0.781027\n",
      "(Iteration 1381 / 1960) loss: 0.819805\n",
      "(Iteration 1401 / 1960) loss: 0.767080\n",
      "(Iteration 1421 / 1960) loss: 1.079360\n",
      "(Iteration 1441 / 1960) loss: 0.774635\n",
      "(Iteration 1461 / 1960) loss: 0.893560\n",
      "(Iteration 1481 / 1960) loss: 0.780504\n",
      "(Iteration 1501 / 1960) loss: 0.890635\n",
      "(Iteration 1521 / 1960) loss: 0.904812\n",
      "(Iteration 1541 / 1960) loss: 0.926377\n",
      "(Iteration 1561 / 1960) loss: 1.175682\n",
      "(Iteration 1581 / 1960) loss: 0.914970\n",
      "(Iteration 1601 / 1960) loss: 0.653017\n",
      "(Iteration 1621 / 1960) loss: 1.044904\n",
      "(Iteration 1641 / 1960) loss: 0.802653\n",
      "(Iteration 1661 / 1960) loss: 0.917531\n",
      "(Iteration 1681 / 1960) loss: 0.874273\n",
      "(Iteration 1701 / 1960) loss: 0.636596\n",
      "(Iteration 1721 / 1960) loss: 0.951595\n",
      "(Iteration 1741 / 1960) loss: 0.873802\n",
      "(Iteration 1761 / 1960) loss: 0.751833\n",
      "(Iteration 1781 / 1960) loss: 0.641927\n",
      "(Iteration 1801 / 1960) loss: 0.716862\n",
      "(Iteration 1821 / 1960) loss: 0.977568\n",
      "(Iteration 1841 / 1960) loss: 0.787588\n",
      "(Iteration 1861 / 1960) loss: 0.671802\n",
      "(Iteration 1881 / 1960) loss: 1.056550\n",
      "(Iteration 1901 / 1960) loss: 0.580803\n",
      "(Iteration 1921 / 1960) loss: 0.713848\n",
      "(Iteration 1941 / 1960) loss: 0.766796\n",
      "(Epoch 2 / 2) train acc: 0.796000; val_acc: 0.591000\n",
      "final accuracy for 40 percent of data, standard model: 0.7960 (train), 0.0000 (test)\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 3 ===\n",
      "(Iteration 1 / 1960) loss: 2.305061\n",
      "(Epoch 0 / 2) train acc: 0.131000; val_acc: 0.151000\n",
      "(Iteration 21 / 1960) loss: 2.095520\n",
      "(Iteration 41 / 1960) loss: 1.968926\n",
      "(Iteration 61 / 1960) loss: 1.862193\n",
      "(Iteration 81 / 1960) loss: 1.674973\n",
      "(Iteration 101 / 1960) loss: 1.566055\n",
      "(Iteration 121 / 1960) loss: 1.785484\n",
      "(Iteration 141 / 1960) loss: 1.767323\n",
      "(Iteration 161 / 1960) loss: 1.768998\n",
      "(Iteration 181 / 1960) loss: 1.391248\n",
      "(Iteration 201 / 1960) loss: 1.785483\n",
      "(Iteration 221 / 1960) loss: 1.559537\n",
      "(Iteration 241 / 1960) loss: 1.872140\n",
      "(Iteration 261 / 1960) loss: 1.516580\n",
      "(Iteration 281 / 1960) loss: 1.429737\n",
      "(Iteration 301 / 1960) loss: 1.405043\n",
      "(Iteration 321 / 1960) loss: 1.910200\n",
      "(Iteration 341 / 1960) loss: 1.357402\n",
      "(Iteration 361 / 1960) loss: 1.337903\n",
      "(Iteration 381 / 1960) loss: 1.410314\n",
      "(Iteration 401 / 1960) loss: 1.360449\n",
      "(Iteration 421 / 1960) loss: 1.311290\n",
      "(Iteration 441 / 1960) loss: 1.443885\n",
      "(Iteration 461 / 1960) loss: 1.278595\n",
      "(Iteration 481 / 1960) loss: 1.212975\n",
      "(Iteration 501 / 1960) loss: 1.229085\n",
      "(Iteration 521 / 1960) loss: 1.375445\n",
      "(Iteration 541 / 1960) loss: 1.445916\n",
      "(Iteration 561 / 1960) loss: 1.379134\n",
      "(Iteration 581 / 1960) loss: 1.429087\n",
      "(Iteration 601 / 1960) loss: 1.249350\n",
      "(Iteration 621 / 1960) loss: 1.362273\n",
      "(Iteration 641 / 1960) loss: 1.268276\n",
      "(Iteration 661 / 1960) loss: 1.437735\n",
      "(Iteration 681 / 1960) loss: 1.255267\n",
      "(Iteration 701 / 1960) loss: 1.193075\n",
      "(Iteration 721 / 1960) loss: 1.288983\n",
      "(Iteration 741 / 1960) loss: 1.220139\n",
      "(Iteration 761 / 1960) loss: 1.247213\n",
      "(Iteration 781 / 1960) loss: 1.087917\n",
      "(Iteration 801 / 1960) loss: 1.243500\n",
      "(Iteration 821 / 1960) loss: 1.249620\n",
      "(Iteration 841 / 1960) loss: 1.206253\n",
      "(Iteration 861 / 1960) loss: 0.951240\n",
      "(Iteration 881 / 1960) loss: 1.338877\n",
      "(Iteration 901 / 1960) loss: 1.392714\n",
      "(Iteration 921 / 1960) loss: 1.003834\n",
      "(Iteration 941 / 1960) loss: 1.107810\n",
      "(Iteration 961 / 1960) loss: 1.110166\n",
      "(Epoch 1 / 2) train acc: 0.626000; val_acc: 0.550000\n",
      "(Iteration 981 / 1960) loss: 1.140023\n",
      "(Iteration 1001 / 1960) loss: 1.191800\n",
      "(Iteration 1021 / 1960) loss: 0.925413\n",
      "(Iteration 1041 / 1960) loss: 1.044394\n",
      "(Iteration 1061 / 1960) loss: 1.028031\n",
      "(Iteration 1081 / 1960) loss: 0.933935\n",
      "(Iteration 1101 / 1960) loss: 0.978463\n",
      "(Iteration 1121 / 1960) loss: 0.738169\n",
      "(Iteration 1141 / 1960) loss: 0.974989\n",
      "(Iteration 1161 / 1960) loss: 0.867268\n",
      "(Iteration 1181 / 1960) loss: 0.846543\n",
      "(Iteration 1201 / 1960) loss: 1.132080\n",
      "(Iteration 1221 / 1960) loss: 1.102483\n",
      "(Iteration 1241 / 1960) loss: 0.871946\n",
      "(Iteration 1261 / 1960) loss: 1.334852\n",
      "(Iteration 1281 / 1960) loss: 0.941864\n",
      "(Iteration 1301 / 1960) loss: 1.125934\n",
      "(Iteration 1321 / 1960) loss: 0.768042\n",
      "(Iteration 1341 / 1960) loss: 0.947643\n",
      "(Iteration 1361 / 1960) loss: 1.141308\n",
      "(Iteration 1381 / 1960) loss: 0.887293\n",
      "(Iteration 1401 / 1960) loss: 0.992583\n",
      "(Iteration 1421 / 1960) loss: 1.212501\n",
      "(Iteration 1441 / 1960) loss: 0.767313\n",
      "(Iteration 1461 / 1960) loss: 1.021159\n",
      "(Iteration 1481 / 1960) loss: 1.004100\n",
      "(Iteration 1501 / 1960) loss: 1.239432\n",
      "(Iteration 1521 / 1960) loss: 0.673640\n",
      "(Iteration 1541 / 1960) loss: 0.821381\n",
      "(Iteration 1561 / 1960) loss: 0.725739\n",
      "(Iteration 1581 / 1960) loss: 0.813948\n",
      "(Iteration 1601 / 1960) loss: 1.309138\n",
      "(Iteration 1621 / 1960) loss: 0.903429\n",
      "(Iteration 1641 / 1960) loss: 0.735755\n",
      "(Iteration 1661 / 1960) loss: 0.789427\n",
      "(Iteration 1681 / 1960) loss: 0.762334\n",
      "(Iteration 1701 / 1960) loss: 0.969663\n",
      "(Iteration 1721 / 1960) loss: 0.626766\n",
      "(Iteration 1741 / 1960) loss: 1.003713\n",
      "(Iteration 1761 / 1960) loss: 0.786388\n",
      "(Iteration 1781 / 1960) loss: 0.762180\n",
      "(Iteration 1801 / 1960) loss: 0.943219\n",
      "(Iteration 1821 / 1960) loss: 0.799849\n",
      "(Iteration 1841 / 1960) loss: 0.958347\n",
      "(Iteration 1861 / 1960) loss: 0.828549\n",
      "(Iteration 1881 / 1960) loss: 0.575651\n",
      "(Iteration 1901 / 1960) loss: 0.858238\n",
      "(Iteration 1921 / 1960) loss: 1.084930\n",
      "(Iteration 1941 / 1960) loss: 0.760444\n",
      "(Epoch 2 / 2) train acc: 0.773000; val_acc: 0.589000\n",
      "final accuracy for 40 percent of data, subspace model: 0.7960 (train), 0.6010 (test)\n",
      "----- trial 4: 20 percent of data (9800 unique examples, 39200 duplicate examples) -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING STANDARD MODEL FOR TRIAL 4 ===\n",
      "(Iteration 1 / 1960) loss: 2.305070\n",
      "(Epoch 0 / 2) train acc: 0.185000; val_acc: 0.167000\n",
      "(Iteration 21 / 1960) loss: 2.180034\n",
      "(Iteration 41 / 1960) loss: 1.956345\n",
      "(Iteration 61 / 1960) loss: 1.936855\n",
      "(Iteration 81 / 1960) loss: 1.659124\n",
      "(Iteration 101 / 1960) loss: 1.657723\n",
      "(Iteration 121 / 1960) loss: 1.629823\n",
      "(Iteration 141 / 1960) loss: 1.465144\n",
      "(Iteration 161 / 1960) loss: 1.739698\n",
      "(Iteration 181 / 1960) loss: 1.372691\n",
      "(Iteration 201 / 1960) loss: 1.452380\n",
      "(Iteration 221 / 1960) loss: 1.336134\n",
      "(Iteration 241 / 1960) loss: 1.469004\n",
      "(Iteration 261 / 1960) loss: 1.384135\n",
      "(Iteration 281 / 1960) loss: 1.280915\n",
      "(Iteration 301 / 1960) loss: 1.172459\n",
      "(Iteration 321 / 1960) loss: 1.383738\n",
      "(Iteration 341 / 1960) loss: 1.382612\n",
      "(Iteration 361 / 1960) loss: 1.187923\n",
      "(Iteration 381 / 1960) loss: 1.547441\n",
      "(Iteration 401 / 1960) loss: 1.355714\n",
      "(Iteration 421 / 1960) loss: 1.397818\n",
      "(Iteration 441 / 1960) loss: 1.235271\n",
      "(Iteration 461 / 1960) loss: 1.006855\n",
      "(Iteration 481 / 1960) loss: 1.045374\n",
      "(Iteration 501 / 1960) loss: 1.008846\n",
      "(Iteration 521 / 1960) loss: 1.265471\n",
      "(Iteration 541 / 1960) loss: 1.053331\n",
      "(Iteration 561 / 1960) loss: 0.839525\n",
      "(Iteration 581 / 1960) loss: 0.988552\n",
      "(Iteration 601 / 1960) loss: 1.084957\n",
      "(Iteration 621 / 1960) loss: 1.112942\n",
      "(Iteration 641 / 1960) loss: 0.851888\n",
      "(Iteration 661 / 1960) loss: 1.103887\n",
      "(Iteration 681 / 1960) loss: 1.211418\n",
      "(Iteration 701 / 1960) loss: 0.866801\n",
      "(Iteration 721 / 1960) loss: 0.834915\n",
      "(Iteration 741 / 1960) loss: 0.975518\n",
      "(Iteration 761 / 1960) loss: 0.933531\n",
      "(Iteration 781 / 1960) loss: 1.130983\n",
      "(Iteration 801 / 1960) loss: 0.953885\n",
      "(Iteration 821 / 1960) loss: 1.104717\n",
      "(Iteration 841 / 1960) loss: 0.942425\n",
      "(Iteration 861 / 1960) loss: 0.788271\n",
      "(Iteration 881 / 1960) loss: 0.790442\n",
      "(Iteration 901 / 1960) loss: 0.938050\n",
      "(Iteration 921 / 1960) loss: 0.873541\n",
      "(Iteration 941 / 1960) loss: 0.799358\n",
      "(Iteration 961 / 1960) loss: 0.679318\n",
      "(Epoch 1 / 2) train acc: 0.749000; val_acc: 0.553000\n",
      "(Iteration 981 / 1960) loss: 0.716088\n",
      "(Iteration 1001 / 1960) loss: 0.498916\n",
      "(Iteration 1021 / 1960) loss: 0.932740\n",
      "(Iteration 1041 / 1960) loss: 0.743314\n",
      "(Iteration 1061 / 1960) loss: 0.688835\n",
      "(Iteration 1081 / 1960) loss: 0.735835\n",
      "(Iteration 1101 / 1960) loss: 0.955120\n",
      "(Iteration 1121 / 1960) loss: 0.687892\n",
      "(Iteration 1141 / 1960) loss: 0.562048\n",
      "(Iteration 1161 / 1960) loss: 0.726413\n",
      "(Iteration 1181 / 1960) loss: 0.728255\n",
      "(Iteration 1201 / 1960) loss: 0.671188\n",
      "(Iteration 1221 / 1960) loss: 0.543888\n",
      "(Iteration 1241 / 1960) loss: 0.575080\n",
      "(Iteration 1261 / 1960) loss: 0.598176\n",
      "(Iteration 1281 / 1960) loss: 0.625159\n",
      "(Iteration 1301 / 1960) loss: 0.431158\n",
      "(Iteration 1321 / 1960) loss: 0.561350\n",
      "(Iteration 1341 / 1960) loss: 0.606866\n",
      "(Iteration 1361 / 1960) loss: 0.490513\n",
      "(Iteration 1381 / 1960) loss: 0.431328\n",
      "(Iteration 1401 / 1960) loss: 0.530085\n",
      "(Iteration 1421 / 1960) loss: 0.387928\n",
      "(Iteration 1441 / 1960) loss: 0.514962\n",
      "(Iteration 1461 / 1960) loss: 0.409856\n",
      "(Iteration 1481 / 1960) loss: 0.344168\n",
      "(Iteration 1501 / 1960) loss: 0.488736\n",
      "(Iteration 1521 / 1960) loss: 0.618127\n",
      "(Iteration 1541 / 1960) loss: 0.415703\n",
      "(Iteration 1561 / 1960) loss: 0.622792\n",
      "(Iteration 1581 / 1960) loss: 0.561198\n",
      "(Iteration 1601 / 1960) loss: 0.404831\n",
      "(Iteration 1621 / 1960) loss: 0.291856\n",
      "(Iteration 1641 / 1960) loss: 0.581437\n",
      "(Iteration 1661 / 1960) loss: 0.514248\n",
      "(Iteration 1681 / 1960) loss: 0.542967\n",
      "(Iteration 1701 / 1960) loss: 0.277156\n",
      "(Iteration 1721 / 1960) loss: 0.740426\n",
      "(Iteration 1741 / 1960) loss: 0.462839\n",
      "(Iteration 1761 / 1960) loss: 0.631374\n",
      "(Iteration 1781 / 1960) loss: 0.320400\n",
      "(Iteration 1801 / 1960) loss: 0.286627\n",
      "(Iteration 1821 / 1960) loss: 0.501248\n",
      "(Iteration 1841 / 1960) loss: 0.324653\n",
      "(Iteration 1861 / 1960) loss: 0.165211\n",
      "(Iteration 1881 / 1960) loss: 0.160924\n",
      "(Iteration 1901 / 1960) loss: 0.251548\n",
      "(Iteration 1921 / 1960) loss: 0.359609\n",
      "(Iteration 1941 / 1960) loss: 0.234985\n",
      "(Epoch 2 / 2) train acc: 0.919000; val_acc: 0.565000\n",
      "final accuracy for 20 percent of data, standard model: 0.9190 (train), 0.0000 (test)\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 4 ===\n",
      "(Iteration 1 / 1960) loss: 2.304998\n",
      "(Epoch 0 / 2) train acc: 0.172000; val_acc: 0.177000\n",
      "(Iteration 21 / 1960) loss: 2.064294\n",
      "(Iteration 41 / 1960) loss: 1.991562\n",
      "(Iteration 61 / 1960) loss: 1.933917\n",
      "(Iteration 81 / 1960) loss: 1.790756\n",
      "(Iteration 101 / 1960) loss: 1.973528\n",
      "(Iteration 121 / 1960) loss: 1.640491\n",
      "(Iteration 141 / 1960) loss: 1.804628\n",
      "(Iteration 161 / 1960) loss: 1.759028\n",
      "(Iteration 181 / 1960) loss: 1.556585\n",
      "(Iteration 201 / 1960) loss: 1.343313\n",
      "(Iteration 221 / 1960) loss: 1.605136\n",
      "(Iteration 241 / 1960) loss: 1.385025\n",
      "(Iteration 261 / 1960) loss: 1.235852\n",
      "(Iteration 281 / 1960) loss: 1.602947\n",
      "(Iteration 301 / 1960) loss: 1.369624\n",
      "(Iteration 321 / 1960) loss: 1.433359\n",
      "(Iteration 341 / 1960) loss: 1.272344\n",
      "(Iteration 361 / 1960) loss: 1.579093\n",
      "(Iteration 381 / 1960) loss: 1.293442\n",
      "(Iteration 401 / 1960) loss: 1.411806\n",
      "(Iteration 421 / 1960) loss: 1.473721\n",
      "(Iteration 441 / 1960) loss: 1.178400\n",
      "(Iteration 461 / 1960) loss: 1.342356\n",
      "(Iteration 481 / 1960) loss: 1.229869\n",
      "(Iteration 501 / 1960) loss: 1.157991\n",
      "(Iteration 521 / 1960) loss: 1.305421\n",
      "(Iteration 541 / 1960) loss: 1.310424\n",
      "(Iteration 561 / 1960) loss: 1.081792\n",
      "(Iteration 581 / 1960) loss: 1.090284\n",
      "(Iteration 601 / 1960) loss: 1.211454\n",
      "(Iteration 621 / 1960) loss: 1.288868\n",
      "(Iteration 641 / 1960) loss: 0.814035\n",
      "(Iteration 661 / 1960) loss: 1.094657\n",
      "(Iteration 681 / 1960) loss: 1.121967\n",
      "(Iteration 701 / 1960) loss: 1.076913\n",
      "(Iteration 721 / 1960) loss: 0.950529\n",
      "(Iteration 741 / 1960) loss: 1.357488\n",
      "(Iteration 761 / 1960) loss: 0.937400\n",
      "(Iteration 781 / 1960) loss: 1.155062\n",
      "(Iteration 801 / 1960) loss: 0.767336\n",
      "(Iteration 821 / 1960) loss: 0.983158\n",
      "(Iteration 841 / 1960) loss: 1.002255\n",
      "(Iteration 861 / 1960) loss: 1.118092\n",
      "(Iteration 881 / 1960) loss: 0.883133\n",
      "(Iteration 901 / 1960) loss: 1.086496\n",
      "(Iteration 921 / 1960) loss: 0.919101\n",
      "(Iteration 941 / 1960) loss: 0.903910\n",
      "(Iteration 961 / 1960) loss: 0.830933\n",
      "(Epoch 1 / 2) train acc: 0.716000; val_acc: 0.545000\n",
      "(Iteration 981 / 1960) loss: 0.842486\n",
      "(Iteration 1001 / 1960) loss: 0.767180\n",
      "(Iteration 1021 / 1960) loss: 0.807080\n",
      "(Iteration 1041 / 1960) loss: 0.774373\n",
      "(Iteration 1061 / 1960) loss: 0.854929\n",
      "(Iteration 1081 / 1960) loss: 0.861606\n",
      "(Iteration 1101 / 1960) loss: 0.712663\n",
      "(Iteration 1121 / 1960) loss: 0.785778\n",
      "(Iteration 1141 / 1960) loss: 0.519656\n",
      "(Iteration 1161 / 1960) loss: 0.835131\n",
      "(Iteration 1181 / 1960) loss: 0.531962\n",
      "(Iteration 1201 / 1960) loss: 0.835850\n",
      "(Iteration 1221 / 1960) loss: 0.676375\n",
      "(Iteration 1241 / 1960) loss: 0.698008\n",
      "(Iteration 1261 / 1960) loss: 0.769688\n",
      "(Iteration 1281 / 1960) loss: 0.708414\n",
      "(Iteration 1301 / 1960) loss: 0.623292\n",
      "(Iteration 1321 / 1960) loss: 0.483025\n",
      "(Iteration 1341 / 1960) loss: 0.543524\n",
      "(Iteration 1361 / 1960) loss: 0.435313\n",
      "(Iteration 1381 / 1960) loss: 0.545230\n",
      "(Iteration 1401 / 1960) loss: 0.689773\n",
      "(Iteration 1421 / 1960) loss: 0.732509\n",
      "(Iteration 1441 / 1960) loss: 0.727112\n",
      "(Iteration 1461 / 1960) loss: 0.479779\n",
      "(Iteration 1481 / 1960) loss: 0.561901\n",
      "(Iteration 1501 / 1960) loss: 0.597303\n",
      "(Iteration 1521 / 1960) loss: 0.445132\n",
      "(Iteration 1541 / 1960) loss: 0.487032\n",
      "(Iteration 1561 / 1960) loss: 0.390748\n",
      "(Iteration 1581 / 1960) loss: 0.581922\n",
      "(Iteration 1601 / 1960) loss: 0.393821\n",
      "(Iteration 1621 / 1960) loss: 0.458023\n",
      "(Iteration 1641 / 1960) loss: 0.528832\n",
      "(Iteration 1661 / 1960) loss: 0.616064\n",
      "(Iteration 1681 / 1960) loss: 0.434872\n",
      "(Iteration 1701 / 1960) loss: 0.426797\n",
      "(Iteration 1721 / 1960) loss: 0.313803\n",
      "(Iteration 1741 / 1960) loss: 0.405979\n",
      "(Iteration 1761 / 1960) loss: 0.416007\n",
      "(Iteration 1781 / 1960) loss: 0.384780\n",
      "(Iteration 1801 / 1960) loss: 0.588965\n",
      "(Iteration 1821 / 1960) loss: 0.376517\n",
      "(Iteration 1841 / 1960) loss: 0.381380\n",
      "(Iteration 1861 / 1960) loss: 0.253604\n",
      "(Iteration 1881 / 1960) loss: 0.322209\n",
      "(Iteration 1901 / 1960) loss: 0.260710\n",
      "(Iteration 1921 / 1960) loss: 0.350039\n",
      "(Iteration 1941 / 1960) loss: 0.276088\n",
      "(Epoch 2 / 2) train acc: 0.915000; val_acc: 0.557000\n",
      "final accuracy for 20 percent of data, subspace model: 0.9190 (train), 0.5620 (test)\n"
     ]
    }
   ],
   "source": [
    "ntrain_total = alldata['X_train'].shape[0]\n",
    "pct_train_sweep = (1, 0.8, 0.6, 0.4, 0.2)\n",
    "results_train_accuracy = np.zeros((len(pct_train_sweep),2))\n",
    "results_test_accuracy  = np.zeros((len(pct_train_sweep),2))\n",
    "for (i,pct_train) in enumerate(pct_train_sweep):\n",
    "    # -------------------------\n",
    "    # --- generate data set ---\n",
    "    # -------------------------\n",
    "    ntrain_unique = round(pct_train*ntrain_total)\n",
    "    ntrain_dupl = ntrain_total - ntrain_unique\n",
    "    ind_unique = np.arange(0,ntrain_unique)\n",
    "    ind_dupl = np.random.choice(np.arange(0,ntrain_unique),size=ntrain_dupl)\n",
    "    ind_train = np.concatenate((ind_unique, ind_dupl))\n",
    "    print('----- trial %d: %d percent of data (%d unique examples, %d duplicate examples) -----' % (i, pct_train*100, ntrain_unique, ntrain_dupl))\n",
    "    #print('unique ind:')\n",
    "    #print(ind_unique)\n",
    "    #print('duplicate ind:')\n",
    "    #print(ind_dupl)\n",
    "    data_abbrev = {\n",
    "        'X_train': deepcopy(alldata['X_train'])[ind_train,:,:,:],\n",
    "        'y_train': deepcopy(alldata['y_train'])[ind_train],\n",
    "        'X_val':   deepcopy(alldata['X_val']),\n",
    "        'y_val':   deepcopy(alldata['y_val']),\n",
    "        'X_test':  deepcopy(alldata['X_test']),\n",
    "        'y_test':  deepcopy(alldata['y_test'])\n",
    "    }\n",
    "    # --------------------------------------\n",
    "    # --- train and report test accuracy ---\n",
    "    # --------------------------------------\n",
    "    standardModel  = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=600, reg=0.001)\n",
    "    subspaceModel  = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=600, reg=0.001)\n",
    "    standardSolver = Solver(standardModel, data_abbrev,\n",
    "                            num_epochs=2, batch_size=50,\n",
    "                            update_rule='adam',\n",
    "                            optim_config={\n",
    "                              'learning_rate': 1e-4,\n",
    "                            },\n",
    "                            verbose=True, print_every=20)\n",
    "    subspaceSolver = Solver(subspaceModel, data_abbrev,\n",
    "                            num_epochs=2, batch_size=50,\n",
    "                            update_rule='adam',\n",
    "                            optim_config={\n",
    "                                'learning_rate': 1e-4,\n",
    "                            },\n",
    "                            verbose=True, print_every=20)\n",
    "    reduced_dim = 24\n",
    "    print('=== TRAINING STANDARD MODEL FOR TRIAL %d ===' % i)\n",
    "    standardSolver.train()\n",
    "    results_train_accuracy[i,0] = standardSolver.train_acc_history[-1]\n",
    "    results_test_accuracy[i,0] = standardSolver.check_accuracy(alldata['X_test'],alldata['y_test'])\n",
    "    print('final accuracy for %d percent of data, standard model: %.4f (train), %.4f (test)' % (pct_train*100, results_train_accuracy[i,0], results_test_accuracy[i,1]))\n",
    "    print('=== TRAINING SUBSPACE MODEL FOR TRIAL %d ===' % i)\n",
    "    subspaceSolver.train(dim=reduced_dim)\n",
    "    results_train_accuracy[i,1] = subspaceSolver.train_acc_history[-1]\n",
    "    results_test_accuracy[i,1] = subspaceSolver.check_accuracy(alldata['X_test'],alldata['y_test'])\n",
    "    print('final accuracy for %d percent of data, subspace model: %.4f (train), %.4f (test)' % (pct_train*100, results_train_accuracy[i,0], results_test_accuracy[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results from 02/26/2018\n",
    "results_train_accuracy = ((0.632, 0.658), (0.692, 0.699), (0.715, 0.701), (0.796, 0.773), (0.919, 0.915))\n",
    "results_test_accuracy  = ((0.611, 0.588), (0.587, 0.609), (0.597, 0.584), (0.591, 0.589), (0.565, 0.557))\n",
    "\n",
    "print('training accuracy:')\n",
    "print(results_train_accuracy)\n",
    "print('test accuracy:')\n",
    "print(results_test_accuracy)\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "set_trace()\n",
    "plt.plot(pct_train_sweep,results_train_accuracy[:,0], '-o')\n",
    "plt.plot(pct_train_sweep,results_train_accuracy[:,1], '-o')\n",
    "plt.legend(['Unconstrained network', 'Subspace constrained network'], loc='upper right')\n",
    "plt.xlabel('portion training samples unique')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlim(0.15,1.05)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(pct_train_sweep,results_test_accuracy[:,0], '-o')\n",
    "plt.plot(pct_train_sweep,results_test_accuracy[:,1], '-o')\n",
    "plt.legend(['Unconstrained network', 'Subspace constrained network'], loc='upper left')\n",
    "plt.xlabel('portion training samples unique')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Test accuracy')\n",
    "plt.xlim(0.15,1.05)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
