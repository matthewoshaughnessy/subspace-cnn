{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "*Question*: how does generalization error (error on validation set) change as we reduce the amount of unique training examples for both the subspace-constrained and non-subspace-constrained methods?\n",
    "*Hypothesis*: Subspace constrained method will have less generalization error than non subspace constrained method as the number of unique training examples decreases\n",
    "*Todo*: use subspace dimension from experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.cnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "from copy import deepcopy\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def rel_error(x, y):\n",
    "  # returns relative error\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "# Load the preprocessed CIFAR10 data.\n",
    "alldata = get_CIFAR10_data()\n",
    "for k, v in alldata.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- trial 0: 100 percent of data (49000 unique examples, 0 duplicate examples) -----\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 0 ===\n",
      "(Iteration 1 / 1960) loss: 2.304941\n",
      "(Epoch 0 / 2) train acc: 0.149000; val_acc: 0.174000\n",
      "(Iteration 21 / 1960) loss: 2.122976\n",
      "(Iteration 41 / 1960) loss: 2.067955\n",
      "(Iteration 61 / 1960) loss: 1.823377\n",
      "(Iteration 81 / 1960) loss: 1.810794\n",
      "(Iteration 101 / 1960) loss: 1.716472\n",
      "(Iteration 121 / 1960) loss: 1.569197\n",
      "(Iteration 141 / 1960) loss: 1.584783\n",
      "(Iteration 161 / 1960) loss: 1.734140\n",
      "(Iteration 181 / 1960) loss: 1.655026\n",
      "(Iteration 201 / 1960) loss: 1.843356\n",
      "(Iteration 221 / 1960) loss: 1.566664\n",
      "(Iteration 241 / 1960) loss: 1.495931\n",
      "(Iteration 261 / 1960) loss: 1.196536\n",
      "(Iteration 281 / 1960) loss: 1.814743\n",
      "(Iteration 301 / 1960) loss: 1.527810\n",
      "(Iteration 321 / 1960) loss: 1.303619\n",
      "(Iteration 341 / 1960) loss: 1.530358\n",
      "(Iteration 361 / 1960) loss: 1.156364\n",
      "(Iteration 381 / 1960) loss: 1.369577\n",
      "(Iteration 401 / 1960) loss: 1.502101\n",
      "(Iteration 421 / 1960) loss: 1.374159\n",
      "(Iteration 441 / 1960) loss: 1.706939\n",
      "(Iteration 461 / 1960) loss: 1.416946\n",
      "(Iteration 481 / 1960) loss: 1.283543\n",
      "(Iteration 501 / 1960) loss: 1.368571\n",
      "(Iteration 521 / 1960) loss: 1.350576\n",
      "(Iteration 541 / 1960) loss: 1.380225\n",
      "(Iteration 561 / 1960) loss: 1.295615\n",
      "(Iteration 581 / 1960) loss: 1.507824\n",
      "(Iteration 601 / 1960) loss: 1.283801\n",
      "(Iteration 621 / 1960) loss: 1.206300\n",
      "(Iteration 641 / 1960) loss: 1.584867\n",
      "(Iteration 661 / 1960) loss: 1.261145\n",
      "(Iteration 681 / 1960) loss: 1.115522\n",
      "(Iteration 701 / 1960) loss: 1.500637\n",
      "(Iteration 721 / 1960) loss: 1.365582\n",
      "(Iteration 741 / 1960) loss: 1.400833\n",
      "(Iteration 761 / 1960) loss: 1.334756\n",
      "(Iteration 781 / 1960) loss: 1.340785\n",
      "(Iteration 801 / 1960) loss: 1.282769\n",
      "(Iteration 821 / 1960) loss: 1.240601\n",
      "(Iteration 841 / 1960) loss: 1.281188\n",
      "(Iteration 861 / 1960) loss: 1.439306\n",
      "(Iteration 881 / 1960) loss: 1.368187\n",
      "(Iteration 901 / 1960) loss: 1.097087\n",
      "(Iteration 921 / 1960) loss: 1.300966\n",
      "(Iteration 941 / 1960) loss: 1.369984\n",
      "(Iteration 961 / 1960) loss: 1.232611\n",
      "(Epoch 1 / 2) train acc: 0.576000; val_acc: 0.581000\n",
      "(Iteration 981 / 1960) loss: 1.107417\n",
      "(Iteration 1001 / 1960) loss: 1.244654\n",
      "(Iteration 1021 / 1960) loss: 1.375770\n",
      "(Iteration 1041 / 1960) loss: 1.188665\n",
      "(Iteration 1061 / 1960) loss: 1.165235\n",
      "(Iteration 1081 / 1960) loss: 1.226685\n",
      "(Iteration 1101 / 1960) loss: 1.369181\n",
      "(Iteration 1121 / 1960) loss: 1.250370\n",
      "(Iteration 1141 / 1960) loss: 1.421089\n",
      "(Iteration 1161 / 1960) loss: 1.326897\n",
      "(Iteration 1181 / 1960) loss: 1.331880\n",
      "(Iteration 1201 / 1960) loss: 1.156459\n",
      "(Iteration 1221 / 1960) loss: 1.127577\n",
      "(Iteration 1241 / 1960) loss: 1.047398\n",
      "(Iteration 1261 / 1960) loss: 1.374049\n",
      "(Iteration 1281 / 1960) loss: 1.153413\n",
      "(Iteration 1301 / 1960) loss: 1.381324\n",
      "(Iteration 1321 / 1960) loss: 1.176013\n",
      "(Iteration 1341 / 1960) loss: 1.018365\n",
      "(Iteration 1361 / 1960) loss: 1.473715\n",
      "(Iteration 1381 / 1960) loss: 1.118413\n",
      "(Iteration 1401 / 1960) loss: 1.283935\n",
      "(Iteration 1421 / 1960) loss: 1.197046\n",
      "(Iteration 1441 / 1960) loss: 1.028558\n",
      "(Iteration 1461 / 1960) loss: 0.838903\n",
      "(Iteration 1481 / 1960) loss: 1.124418\n",
      "(Iteration 1501 / 1960) loss: 1.043867\n",
      "(Iteration 1521 / 1960) loss: 1.280943\n",
      "(Iteration 1541 / 1960) loss: 0.726752\n",
      "(Iteration 1561 / 1960) loss: 1.128552\n",
      "(Iteration 1581 / 1960) loss: 1.040941\n",
      "(Iteration 1601 / 1960) loss: 1.343108\n",
      "(Iteration 1621 / 1960) loss: 1.085479\n",
      "(Iteration 1641 / 1960) loss: 0.979001\n",
      "(Iteration 1661 / 1960) loss: 1.189362\n",
      "(Iteration 1681 / 1960) loss: 1.342350\n",
      "(Iteration 1701 / 1960) loss: 1.108889\n",
      "(Iteration 1721 / 1960) loss: 1.044750\n",
      "(Iteration 1741 / 1960) loss: 0.964528\n",
      "(Iteration 1761 / 1960) loss: 1.004978\n",
      "(Iteration 1781 / 1960) loss: 0.998037\n",
      "(Iteration 1801 / 1960) loss: 1.176341\n",
      "(Iteration 1821 / 1960) loss: 1.099829\n",
      "(Iteration 1841 / 1960) loss: 1.190846\n",
      "(Iteration 1861 / 1960) loss: 0.862214\n",
      "(Iteration 1881 / 1960) loss: 1.246191\n",
      "(Iteration 1901 / 1960) loss: 1.017748\n",
      "(Iteration 1921 / 1960) loss: 1.159713\n",
      "(Iteration 1941 / 1960) loss: 1.035718\n",
      "(Epoch 2 / 2) train acc: 0.648000; val_acc: 0.600000\n",
      "final accuracy for 100 percent of data, subspace model: 0.0000 (train), 0.5990 (test)\n",
      "----- trial 1: 80 percent of data (39200 unique examples, 9800 duplicate examples) -----\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 1 ===\n",
      "(Iteration 1 / 1960) loss: 2.305204\n",
      "(Epoch 0 / 2) train acc: 0.146000; val_acc: 0.142000\n",
      "(Iteration 21 / 1960) loss: 1.884600\n",
      "(Iteration 41 / 1960) loss: 1.952960\n",
      "(Iteration 61 / 1960) loss: 1.758385\n",
      "(Iteration 81 / 1960) loss: 1.955527\n",
      "(Iteration 101 / 1960) loss: 1.547280\n",
      "(Iteration 121 / 1960) loss: 1.646398\n",
      "(Iteration 141 / 1960) loss: 1.662275\n",
      "(Iteration 161 / 1960) loss: 1.738395\n",
      "(Iteration 181 / 1960) loss: 1.531467\n",
      "(Iteration 201 / 1960) loss: 1.561395\n",
      "(Iteration 221 / 1960) loss: 1.693728\n",
      "(Iteration 241 / 1960) loss: 1.308750\n",
      "(Iteration 261 / 1960) loss: 1.472563\n",
      "(Iteration 281 / 1960) loss: 1.428199\n",
      "(Iteration 301 / 1960) loss: 1.517911\n",
      "(Iteration 321 / 1960) loss: 1.121630\n",
      "(Iteration 341 / 1960) loss: 1.270393\n",
      "(Iteration 361 / 1960) loss: 1.529275\n",
      "(Iteration 381 / 1960) loss: 1.384489\n",
      "(Iteration 401 / 1960) loss: 1.331413\n",
      "(Iteration 421 / 1960) loss: 1.422858\n",
      "(Iteration 441 / 1960) loss: 1.455121\n",
      "(Iteration 461 / 1960) loss: 1.376341\n",
      "(Iteration 481 / 1960) loss: 1.707436\n",
      "(Iteration 501 / 1960) loss: 1.297932\n",
      "(Iteration 521 / 1960) loss: 1.655299\n",
      "(Iteration 541 / 1960) loss: 1.442451\n",
      "(Iteration 561 / 1960) loss: 1.371720\n",
      "(Iteration 581 / 1960) loss: 1.611554\n",
      "(Iteration 601 / 1960) loss: 1.218309\n",
      "(Iteration 621 / 1960) loss: 1.211345\n",
      "(Iteration 641 / 1960) loss: 1.268696\n",
      "(Iteration 661 / 1960) loss: 1.311731\n",
      "(Iteration 681 / 1960) loss: 1.273802\n",
      "(Iteration 701 / 1960) loss: 1.351634\n",
      "(Iteration 721 / 1960) loss: 1.120871\n",
      "(Iteration 741 / 1960) loss: 1.447006\n",
      "(Iteration 761 / 1960) loss: 1.367024\n",
      "(Iteration 781 / 1960) loss: 1.379193\n",
      "(Iteration 801 / 1960) loss: 1.218116\n",
      "(Iteration 821 / 1960) loss: 1.360475\n",
      "(Iteration 841 / 1960) loss: 1.385171\n",
      "(Iteration 861 / 1960) loss: 1.213707\n",
      "(Iteration 881 / 1960) loss: 1.487964\n",
      "(Iteration 901 / 1960) loss: 0.926468\n",
      "(Iteration 921 / 1960) loss: 1.147684\n",
      "(Iteration 941 / 1960) loss: 1.383152\n",
      "(Iteration 961 / 1960) loss: 1.355154\n",
      "(Epoch 1 / 2) train acc: 0.587000; val_acc: 0.558000\n",
      "(Iteration 981 / 1960) loss: 1.374932\n",
      "(Iteration 1001 / 1960) loss: 1.308645\n",
      "(Iteration 1021 / 1960) loss: 1.214718\n",
      "(Iteration 1041 / 1960) loss: 0.906691\n",
      "(Iteration 1061 / 1960) loss: 1.236544\n",
      "(Iteration 1081 / 1960) loss: 1.163543\n",
      "(Iteration 1101 / 1960) loss: 1.240261\n",
      "(Iteration 1121 / 1960) loss: 1.400090\n",
      "(Iteration 1141 / 1960) loss: 1.524693\n",
      "(Iteration 1161 / 1960) loss: 1.232320\n",
      "(Iteration 1181 / 1960) loss: 1.076220\n",
      "(Iteration 1201 / 1960) loss: 1.243772\n",
      "(Iteration 1221 / 1960) loss: 1.080216\n",
      "(Iteration 1241 / 1960) loss: 1.044580\n",
      "(Iteration 1261 / 1960) loss: 1.417077\n",
      "(Iteration 1281 / 1960) loss: 0.871197\n",
      "(Iteration 1301 / 1960) loss: 0.945996\n",
      "(Iteration 1321 / 1960) loss: 0.992815\n",
      "(Iteration 1341 / 1960) loss: 1.241143\n",
      "(Iteration 1361 / 1960) loss: 1.178509\n",
      "(Iteration 1381 / 1960) loss: 1.205556\n",
      "(Iteration 1401 / 1960) loss: 1.108334\n",
      "(Iteration 1421 / 1960) loss: 1.085257\n",
      "(Iteration 1441 / 1960) loss: 0.814626\n",
      "(Iteration 1461 / 1960) loss: 1.027116\n",
      "(Iteration 1481 / 1960) loss: 1.128914\n",
      "(Iteration 1501 / 1960) loss: 1.179007\n",
      "(Iteration 1521 / 1960) loss: 1.238512\n",
      "(Iteration 1541 / 1960) loss: 0.966057\n",
      "(Iteration 1561 / 1960) loss: 1.102880\n",
      "(Iteration 1581 / 1960) loss: 0.820869\n",
      "(Iteration 1601 / 1960) loss: 1.043944\n",
      "(Iteration 1621 / 1960) loss: 1.053094\n",
      "(Iteration 1641 / 1960) loss: 1.053563\n",
      "(Iteration 1661 / 1960) loss: 1.151395\n",
      "(Iteration 1681 / 1960) loss: 1.029544\n",
      "(Iteration 1701 / 1960) loss: 1.299736\n",
      "(Iteration 1721 / 1960) loss: 1.042378\n",
      "(Iteration 1741 / 1960) loss: 1.203637\n",
      "(Iteration 1761 / 1960) loss: 1.020617\n",
      "(Iteration 1781 / 1960) loss: 0.990053\n",
      "(Iteration 1801 / 1960) loss: 1.021240\n",
      "(Iteration 1821 / 1960) loss: 0.896799\n",
      "(Iteration 1841 / 1960) loss: 1.248181\n",
      "(Iteration 1861 / 1960) loss: 0.854722\n",
      "(Iteration 1881 / 1960) loss: 1.168212\n",
      "(Iteration 1901 / 1960) loss: 1.151644\n",
      "(Iteration 1921 / 1960) loss: 1.035363\n",
      "(Iteration 1941 / 1960) loss: 1.071396\n",
      "(Epoch 2 / 2) train acc: 0.687000; val_acc: 0.621000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final accuracy for 80 percent of data, subspace model: 0.0000 (train), 0.5990 (test)\n",
      "----- trial 2: 60 percent of data (29400 unique examples, 19600 duplicate examples) -----\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 2 ===\n",
      "(Iteration 1 / 1960) loss: 2.304823\n",
      "(Epoch 0 / 2) train acc: 0.161000; val_acc: 0.172000\n",
      "(Iteration 21 / 1960) loss: 2.150688\n",
      "(Iteration 41 / 1960) loss: 1.913817\n",
      "(Iteration 61 / 1960) loss: 1.730101\n",
      "(Iteration 81 / 1960) loss: 2.106959\n",
      "(Iteration 101 / 1960) loss: 1.609115\n",
      "(Iteration 121 / 1960) loss: 1.680619\n",
      "(Iteration 141 / 1960) loss: 1.521596\n",
      "(Iteration 161 / 1960) loss: 1.473819\n",
      "(Iteration 181 / 1960) loss: 1.644291\n",
      "(Iteration 201 / 1960) loss: 1.432734\n",
      "(Iteration 221 / 1960) loss: 1.500182\n",
      "(Iteration 241 / 1960) loss: 1.781468\n",
      "(Iteration 261 / 1960) loss: 1.655789\n",
      "(Iteration 281 / 1960) loss: 1.560181\n",
      "(Iteration 301 / 1960) loss: 1.538825\n",
      "(Iteration 321 / 1960) loss: 1.660952\n",
      "(Iteration 341 / 1960) loss: 1.469213\n",
      "(Iteration 361 / 1960) loss: 1.428372\n",
      "(Iteration 381 / 1960) loss: 1.341191\n",
      "(Iteration 401 / 1960) loss: 1.262069\n",
      "(Iteration 421 / 1960) loss: 1.454598\n",
      "(Iteration 441 / 1960) loss: 1.232757\n",
      "(Iteration 461 / 1960) loss: 1.591714\n",
      "(Iteration 481 / 1960) loss: 1.449482\n",
      "(Iteration 501 / 1960) loss: 1.142513\n",
      "(Iteration 521 / 1960) loss: 1.388473\n",
      "(Iteration 541 / 1960) loss: 1.068380\n",
      "(Iteration 561 / 1960) loss: 1.351962\n",
      "(Iteration 581 / 1960) loss: 1.430425\n",
      "(Iteration 601 / 1960) loss: 1.360688\n",
      "(Iteration 621 / 1960) loss: 1.442095\n",
      "(Iteration 641 / 1960) loss: 1.216242\n",
      "(Iteration 661 / 1960) loss: 1.174820\n",
      "(Iteration 681 / 1960) loss: 1.064228\n",
      "(Iteration 701 / 1960) loss: 1.532936\n",
      "(Iteration 721 / 1960) loss: 1.342770\n",
      "(Iteration 741 / 1960) loss: 1.301506\n",
      "(Iteration 761 / 1960) loss: 1.383234\n",
      "(Iteration 781 / 1960) loss: 1.426437\n",
      "(Iteration 801 / 1960) loss: 1.162844\n",
      "(Iteration 821 / 1960) loss: 1.240756\n",
      "(Iteration 841 / 1960) loss: 1.116384\n",
      "(Iteration 861 / 1960) loss: 1.387832\n",
      "(Iteration 881 / 1960) loss: 1.229611\n",
      "(Iteration 901 / 1960) loss: 1.318652\n",
      "(Iteration 921 / 1960) loss: 1.307122\n",
      "(Iteration 941 / 1960) loss: 1.251349\n",
      "(Iteration 961 / 1960) loss: 1.227521\n",
      "(Epoch 1 / 2) train acc: 0.597000; val_acc: 0.556000\n",
      "(Iteration 981 / 1960) loss: 1.183525\n",
      "(Iteration 1001 / 1960) loss: 1.025848\n",
      "(Iteration 1021 / 1960) loss: 0.913617\n",
      "(Iteration 1041 / 1960) loss: 1.096042\n",
      "(Iteration 1061 / 1960) loss: 1.146762\n",
      "(Iteration 1081 / 1960) loss: 1.340360\n",
      "(Iteration 1101 / 1960) loss: 0.938939\n",
      "(Iteration 1121 / 1960) loss: 1.179375\n",
      "(Iteration 1141 / 1960) loss: 1.296124\n",
      "(Iteration 1161 / 1960) loss: 0.779078\n",
      "(Iteration 1181 / 1960) loss: 1.061383\n",
      "(Iteration 1201 / 1960) loss: 1.441833\n",
      "(Iteration 1221 / 1960) loss: 1.038146\n",
      "(Iteration 1241 / 1960) loss: 1.066045\n",
      "(Iteration 1261 / 1960) loss: 1.195706\n",
      "(Iteration 1281 / 1960) loss: 1.008148\n",
      "(Iteration 1301 / 1960) loss: 0.918878\n",
      "(Iteration 1321 / 1960) loss: 1.376759\n",
      "(Iteration 1341 / 1960) loss: 1.008921\n",
      "(Iteration 1361 / 1960) loss: 0.952287\n",
      "(Iteration 1381 / 1960) loss: 0.933592\n",
      "(Iteration 1401 / 1960) loss: 0.929592\n",
      "(Iteration 1421 / 1960) loss: 0.898485\n",
      "(Iteration 1441 / 1960) loss: 0.999373\n",
      "(Iteration 1461 / 1960) loss: 0.868104\n",
      "(Iteration 1481 / 1960) loss: 0.822111\n",
      "(Iteration 1501 / 1960) loss: 0.927096\n",
      "(Iteration 1521 / 1960) loss: 0.908291\n",
      "(Iteration 1541 / 1960) loss: 1.094269\n",
      "(Iteration 1561 / 1960) loss: 1.101549\n",
      "(Iteration 1581 / 1960) loss: 1.048082\n",
      "(Iteration 1601 / 1960) loss: 1.148625\n",
      "(Iteration 1621 / 1960) loss: 0.793528\n",
      "(Iteration 1641 / 1960) loss: 1.010526\n",
      "(Iteration 1661 / 1960) loss: 0.842592\n",
      "(Iteration 1681 / 1960) loss: 1.050006\n",
      "(Iteration 1701 / 1960) loss: 1.118198\n",
      "(Iteration 1721 / 1960) loss: 0.986360\n",
      "(Iteration 1741 / 1960) loss: 0.621354\n",
      "(Iteration 1761 / 1960) loss: 1.051182\n",
      "(Iteration 1781 / 1960) loss: 1.050775\n",
      "(Iteration 1801 / 1960) loss: 0.809569\n",
      "(Iteration 1821 / 1960) loss: 1.012107\n",
      "(Iteration 1841 / 1960) loss: 0.872305\n",
      "(Iteration 1861 / 1960) loss: 0.991163\n",
      "(Iteration 1881 / 1960) loss: 0.682418\n",
      "(Iteration 1901 / 1960) loss: 0.818837\n",
      "(Iteration 1921 / 1960) loss: 1.103715\n",
      "(Iteration 1941 / 1960) loss: 1.046254\n",
      "(Epoch 2 / 2) train acc: 0.702000; val_acc: 0.625000\n",
      "final accuracy for 60 percent of data, subspace model: 0.0000 (train), 0.5930 (test)\n",
      "----- trial 3: 40 percent of data (19600 unique examples, 29400 duplicate examples) -----\n",
      "=== TRAINING SUBSPACE MODEL FOR TRIAL 3 ===\n",
      "(Iteration 1 / 1960) loss: 2.305057\n",
      "(Epoch 0 / 2) train acc: 0.172000; val_acc: 0.174000\n",
      "(Iteration 21 / 1960) loss: 2.203697\n",
      "(Iteration 41 / 1960) loss: 2.000119\n",
      "(Iteration 61 / 1960) loss: 2.051418\n",
      "(Iteration 81 / 1960) loss: 1.946763\n",
      "(Iteration 101 / 1960) loss: 1.844129\n",
      "(Iteration 121 / 1960) loss: 1.685014\n",
      "(Iteration 141 / 1960) loss: 1.706290\n",
      "(Iteration 161 / 1960) loss: 1.825018\n",
      "(Iteration 181 / 1960) loss: 1.681457\n",
      "(Iteration 201 / 1960) loss: 1.696826\n",
      "(Iteration 221 / 1960) loss: 1.623292\n",
      "(Iteration 241 / 1960) loss: 1.661825\n",
      "(Iteration 261 / 1960) loss: 1.632883\n",
      "(Iteration 281 / 1960) loss: 1.441438\n"
     ]
    }
   ],
   "source": [
    "ntrain_total = alldata['X_train'].shape[0]\n",
    "pct_train_sweep = (1, 0.8, 0.6, 0.4, 0.2)\n",
    "results_train_accuracy = np.zeros((len(pct_train_sweep),2))\n",
    "results_test_accuracy  = np.zeros((len(pct_train_sweep),2))\n",
    "for (i,pct_train) in enumerate(pct_train_sweep):\n",
    "    # -------------------------\n",
    "    # --- generate data set ---\n",
    "    # -------------------------\n",
    "    ntrain_unique = round(pct_train*ntrain_total)\n",
    "    ntrain_dupl = ntrain_total - ntrain_unique\n",
    "    ind_unique = np.arange(0,ntrain_unique)\n",
    "    ind_dupl = np.random.choice(np.arange(0,ntrain_unique),size=ntrain_dupl)\n",
    "    ind_train = np.concatenate((ind_unique, ind_dupl))\n",
    "    print('----- trial %d: %d percent of data (%d unique examples, %d duplicate examples) -----' % (i, pct_train*100, ntrain_unique, ntrain_dupl))\n",
    "    #print('unique ind:')\n",
    "    #print(ind_unique)\n",
    "    #print('duplicate ind:')\n",
    "    #print(ind_dupl)\n",
    "    data_abbrev = {\n",
    "        'X_train': deepcopy(alldata['X_train'])[ind_train,:,:,:],\n",
    "        'y_train': deepcopy(alldata['y_train'])[ind_train],\n",
    "        'X_val':   deepcopy(alldata['X_val']),\n",
    "        'y_val':   deepcopy(alldata['y_val']),\n",
    "        'X_test':  deepcopy(alldata['X_test']),\n",
    "        'y_test':  deepcopy(alldata['y_test'])\n",
    "    }\n",
    "    # --------------------------------------\n",
    "    # --- train and report test accuracy ---\n",
    "    # --------------------------------------\n",
    "    standardModel  = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=600, reg=0.001)\n",
    "    subspaceModel  = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=600, reg=0.001)\n",
    "    standardSolver = Solver(standardModel, data_abbrev,\n",
    "                            num_epochs=2, batch_size=50,\n",
    "                            update_rule='adam',\n",
    "                            optim_config={\n",
    "                              'learning_rate': 1e-4,\n",
    "                            },\n",
    "                            verbose=True, print_every=20)\n",
    "    subspaceSolver = Solver(subspaceModel, data_abbrev,\n",
    "                            num_epochs=2, batch_size=50,\n",
    "                            update_rule='adam',\n",
    "                            optim_config={\n",
    "                                'learning_rate': 1e-4,\n",
    "                            },\n",
    "                            verbose=True, print_every=20)\n",
    "    ambient_dim = 48 # TODO 49 makes value of loss function blow up (or 48)\n",
    "    reduced_dim = 24\n",
    "    #print('=== TRAINING STANDARD MODEL FOR TRIAL %d ===' % i)\n",
    "    #standardSolver.train(dim=ambient_dim)\n",
    "    #results_train_accuracy[i,0] = standardSolver.train_acc_history[-1]\n",
    "    #results_test_accuracy[i,0] = subspaceSolver.check_accuracy(alldata['X_test'],alldata['y_test'])\n",
    "    #print('final accuracy for %d percent of data, standard model: %.4f (train), %.4f (test)' % (pct_train*100, results_train_accuracy[i,0], results_test_accuracy[i,1]))\n",
    "    print('=== TRAINING SUBSPACE MODEL FOR TRIAL %d ===' % i)\n",
    "    subspaceSolver.train(dim=reduced_dim)\n",
    "    results_train_accuracy[i,1] = subspaceSolver.train_acc_history[-1]\n",
    "    results_test_accuracy[i,1] = subspaceSolver.check_accuracy(alldata['X_test'],alldata['y_test'])\n",
    "    print('final accuracy for %d percent of data, subspace model: %.4f (train), %.4f (test)' % (pct_train*100, results_train_accuracy[i,0], results_test_accuracy[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_train_accuracy = np.random.rand(6,2)\n",
    "#results_test_accuracy = np.random.rand(6,2)\n",
    "print(results_train_accuracy)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(results_train_accuracy[:,0], '-o')\n",
    "plt.plot(results_train_accuracy[:,1], '-o')\n",
    "plt.legend(['0', '1'], loc='upper left')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(results_test_accuracy[:,0], '-o')\n",
    "plt.plot(results_test_accuracy[:,1], '-o')\n",
    "plt.legend(['0','1'], loc='upper left')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
